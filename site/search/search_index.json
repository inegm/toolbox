{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Toolbox Implementations, mostly pedagogical, of various algorithms and data-structures. The domains of interest range from classic algorithms and data-structures to machine-learning and AI through quantitative finance and distributed systems. The primary intention of this library is to develop, in Python and using only the standard library and common external libraries (numpy, pandas, for example), simple implementations of algorithms in common use. Often, but not always, there is a better performing implementation available in open-source or in a higher performance language. My intention here is not to replace them, but to simplify them. Together, the documentation and the code should yield something of a clear pedagogical narrative. Setting-up This library will be made available on PyPI once I've settled on a decent number of modules for a first package. On the horizon Artificial Neural Networks Reinforcement Learning (Q-learning) Ant-colony optimization Evolutionary algorithms for optimization Financial market-risk metrics (VaR, ES, etc) Various auction mechanisms MCMC methods GARCH processes Clock synchronization (from the SNTP) Best master clock (from the PTP)","title":"About"},{"location":"#toolbox","text":"Implementations, mostly pedagogical, of various algorithms and data-structures. The domains of interest range from classic algorithms and data-structures to machine-learning and AI through quantitative finance and distributed systems. The primary intention of this library is to develop, in Python and using only the standard library and common external libraries (numpy, pandas, for example), simple implementations of algorithms in common use. Often, but not always, there is a better performing implementation available in open-source or in a higher performance language. My intention here is not to replace them, but to simplify them. Together, the documentation and the code should yield something of a clear pedagogical narrative.","title":"Toolbox"},{"location":"#setting-up","text":"This library will be made available on PyPI once I've settled on a decent number of modules for a first package.","title":"Setting-up"},{"location":"#on-the-horizon","text":"Artificial Neural Networks Reinforcement Learning (Q-learning) Ant-colony optimization Evolutionary algorithms for optimization Financial market-risk metrics (VaR, ES, etc) Various auction mechanisms MCMC methods GARCH processes Clock synchronization (from the SNTP) Best master clock (from the PTP)","title":"On the horizon"},{"location":"machine-learning/decision-trees/","text":"Decision Trees DecisionTreeClassifier Caveat This is a simple, pedagogical implementation of a decision-tree classifier. It is not likely to win you any Kaggle competitions on its own. It typically achieves accuracy scores around 0.7 and is slow to train. That said, this is what RandomForest, Ensemble methods, and XGBoost are based on, so it's well worth understanding how it works. Fitting the tree These classifiers work by recursively splitting a dataset using queries on the features in such a way as to maximize information gain at each new split. Information gain is calculated for each split by using an impurity function , which is sometimes known as an error function . Queries can be as complex as the implementer cares to make them, but in this implementation they are simple : all queries here are of the form \\(v \\ge w\\) . A value \\(w\\) is selected from the features and a candidate split is created : all examples for which the query yields True go to the left child and all others go to the right child. The candidate information gain is then evaluated. This process is repeated for each feature and for each unique feature value. The actual split is chosen to be the candidate with the highest information gain. For more on information gain, see the information_gain method's documentation. The leaves of the tree contain subsets of the dataset with no impurity, that is to say all examples of the subset belong to a single class. Making predictions By traveling through the tree with a novel, unlabeled example, applying the queries to its features' values to determine the path, we end up at a leaf node which determines the predicted class label for the novel example. Examples: >>> import pandas as pd >>> url = \"/\" . join ([ \"https://archive.ics.uci.edu\" , \"ml/machine-learning-databases\" , \"heart-disease/processed.cleveland.data\" ]) >>> df = pd . read_csv ( url , header = None , names = [ \"age\" , \"sex\" , \"cp\" , \"trestbps\" , \"chol\" , \"fbs\" , \"restecg\" , \"thalach\" , \"exang\" , \"oldpeak\" , \"slope\" , \"ca\" , \"thal\" , \"target\" ] ) >>> df [ 'target' ] . replace ({ 2 : 1 , 3 : 1 , 4 : 1 }, inplace = True ) >>> train = df [: 201 ] >>> test = df [ 201 :] >>> dtc = DecisionTreeClassifier ( train , 'target' , gini_impurity ) >>> dtc . fit () # 2 minute coffee break >>> _ = dtc . evaluate ( test ) MODEL Precision mean 0.688657 weighted 0.696078 Precision negative mean 0.688657 weighted 0.681236 Sensitivity mean 0.703326 weighted 0.701773 Specificity mean 0.703326 weighted 0.704880 Accuracy mean 0.696078 weighted 0.696078 F1-score mean 0.687395 weighted 0.690460 dtype: float64 CLASSES 0 1 True Positives 44.000000 27.000000 True Negatives 27.000000 44.000000 False Positives 10.000000 21.000000 False Negatives 21.000000 10.000000 Precision 0.814815 0.562500 Precision negative 0.562500 0.814815 Sensitivity 0.676923 0.729730 Specificity 0.729730 0.676923 Accuracy 0.696078 0.696078 F1-score 0.739496 0.635294 Weight (actual) 0.529412 0.470588 CONFUSION MATRIX 0 1 actual 0 44 10 1 21 27 A comparison The accuracy score for the given example is of 0.70, which is not far from scikit-learn's DecisionTreeClassifier accuracy of 0.76 given the same dataset. That said, with hyperparameter tuning (not possible here), the scikit-learn implementation can reach much higher accuracy scores (above 0.85). tree property readonly Pointer to the decision-tree's root node. The (binary) tree can be traversed by traveling through each node's left (True) and right (False) child nodes. __init__ ( self , dataset , labels_col , impurity_func ) special Parameters: Name Type Description Default dataset pd.DataFrame Including one or more feature columns and a single labels column. required labels_col str The name of the labels column. required impurity_func Callable Options include gini_impurity and entropy , but any callable with the same signature which returns a float will do. required Source code in toolbox/algorithms/learning/decisiontree.py def __init__ ( self , dataset : pd . DataFrame , labels_col : str , impurity_func : Callable , ): \"\"\" Args: dataset: Including one or more feature columns and a single labels column. labels_col: The name of the labels column. impurity_func: Options include [gini_impurity] [toolbox.algorithms.learning.decisiontree.gini_impurity] and [entropy][toolbox.algorithms.learning.decisiontree.entropy], but any callable with the same signature which returns a float will do. \"\"\" self . features = dataset . drop ( labels_col , axis = 1 ) self . labels = dataset [ labels_col ] self . impurity_func = impurity_func self . _tree = None evaluate ( self , data , verbose = True ) Evaluate the performance of the decision-tree model. See evaluation.evaluate_classifier . Parameters: Name Type Description Default data pd.DataFrame A labeled DataFrame. The label column is expected to be named the same as the labels Series that was provided at initialization. required Source code in toolbox/algorithms/learning/decisiontree.py def evaluate ( self , data : pd . DataFrame , verbose = True , ) -> Tuple [ pd . Series , pd . DataFrame , pd . DataFrame ]: \"\"\"Evaluate the performance of the decision-tree model. See [evaluation.evaluate_classifier] [toolbox.algorithms.learning.evaluation.evaluate_classifier]. Args: data: A labeled DataFrame. The label column is expected to be named the same as the labels Series that was provided at initialization. \"\"\" predict_col = str ( self . labels . name ) actual_col = \"_\" . join ([ predict_col , \"actual\" ]) pred_df = self . predict ( data . drop ( predict_col , axis = 1 )) pred_df [ actual_col ] = data [ predict_col ] model_eval , class_eval , confusion_matrix = evaluate_classifier ( predict_df = pred_df , predict_col = predict_col , actual_col = actual_col , verbose = verbose , ) return ( model_eval , class_eval , confusion_matrix ) find_best_split ( self , idx ) Find the split which yields the highest information gain. Parameters: Name Type Description Default idx Any The (pandas array) index of the examples of the node before the split. required Source code in toolbox/algorithms/learning/decisiontree.py def find_best_split ( self , idx : Any ): \"\"\"Find the split which yields the highest information gain. Args: idx (pd.array): The (pandas array) index of the examples of the node before the split. \"\"\" max_gain = 0.0 query = None true_idx = None false_idx = None for feature in self . features . columns : for value in self . features [ feature ] . unique (): split_condition = self . features . loc [ idx ][ feature ] . ge ( value ) n_true = split_condition . value_counts () . get ( True , 0 ) n_false = split_condition . value_counts () . get ( False , 0 ) if ( n_true == 0 ) or ( n_false == 0 ): continue gain = self . information_gain ( idx = idx , true_idx = split_condition ) if gain >= max_gain : true_idx = self . features . loc [ idx ][ split_condition ] . index false_idx = self . features . loc [ idx ][ ~ split_condition ] . index max_gain = gain query = DTCQuery ( feature , value , ge ) return ( max_gain , query , true_idx , false_idx ) fit ( self ) Build the decision tree. Source code in toolbox/algorithms/learning/decisiontree.py def fit ( self ): \"\"\"Build the decision tree.\"\"\" def _fit ( node ): gain , query , left_idx , right_idx = self . find_best_split ( node . idx ) if gain == 0 : return DTCNode ( node . idx , leaf = True , label = self . labels . loc [ node . idx ] . values [ 0 ] ) node . query = query node . left = _fit ( DTCNode ( left_idx )) node . right = _fit ( DTCNode ( right_idx )) return node self . _tree = _fit ( DTCNode ( self . features . index )) information_gain ( self , idx , true_idx ) Gain in information resulting from a split of the parent node. Weighted impurity is calculated for each side of the split, using the impurity function set at the initialization of the model, and the total impurity of the split is determined by the sum of these. The information gain is the difference of the impurity of the parent and this sum : \\(gain = I_{p} - w_{l}I_{l} + w_{r}I_{r}\\) The weights are calculated as the fraction of unique labels present in the split relative to the model's set of all known labels. Parameters: Name Type Description Default idx Any The (pandas array) index of the examples of the node before the split. required true_idx Any The (pandas array) index of the node's examples for which the query condition is true. ie.: The left child's index after the split. required Source code in toolbox/algorithms/learning/decisiontree.py def information_gain ( self , idx : Any , true_idx : Any ) -> float : \"\"\"Gain in information resulting from a split of the parent node. Weighted impurity is calculated for each side of the split, using the impurity function set at the initialization of the model, and the total impurity of the split is determined by the sum of these. The information gain is the difference of the impurity of the parent and this sum : $gain = I_{p} - w_{l}I_{l} + w_{r}I_{r}$ The weights are calculated as the fraction of unique labels present in the split relative to the model's set of all known labels. Args: idx (pd.array): The (pandas array) index of the examples of the node before the split. true_idx (pd.array): The (pandas array) index of the node's examples for which the query condition is true. ie.: The left child's index after the split. \"\"\" def _calc_impurity ( values : pd . Series , n_total : int , impurity_func : Callable , ) -> float : return len ( values ) / n_total * impurity_func ( values ) labels = self . labels . loc [ idx ] n_total = len ( labels ) true_impurity = _calc_impurity ( labels [ true_idx ], n_total , self . impurity_func ) false_impurity = _calc_impurity ( labels [ ~ true_idx ], n_total , self . impurity_func ) total_impurity = true_impurity + false_impurity return self . impurity_func ( labels ) - total_impurity predict ( self , data ) Predict classes for a set of unlabeled examples. Parameters: Name Type Description Default data pd.DataFrame Unlabeled DataFrame of examples. required Source code in toolbox/algorithms/learning/decisiontree.py def predict ( self , data : pd . DataFrame ): \"\"\"Predict classes for a set of unlabeled examples. Args: data: Unlabeled DataFrame of examples. \"\"\" labels = list () for _ix , row in data . iterrows (): node = self . tree while not node . leaf : if node . query . operator ( row [ node . query . feature ], node . query . value ): node = node . left else : node = node . right labels . append ( node . label ) data [ self . labels . name ] = labels return data gini_impurity ( labels ) Gini impurity, a measure of uncertainty used by the CART algorithm. Parameters: Name Type Description Default labels Union[List[Any], pd.Series] The labels column of the dataset required A measure of the probability that a label selected at random from the distribution of labels in a dataset would incorrectly label an example from that dataset. A Gini impurity of 0 indicates a perfectly certain dataset, that is one with a single unique label. The measure is calculated as 1 minus the sum of squared probabilities for each unique label. \\(I_{G}(p) = 1 - \\sum_{i=1}^{n}p_{i}^2\\) for \\(\\mathnormal{n}\\) classes with \\(i \\in \\{1, 2, ..., n\\}\\) Source code in toolbox/algorithms/learning/decisiontree.py def gini_impurity ( labels : Union [ List [ Any ], pd . Series ]) -> float : \"\"\"Gini impurity, a measure of uncertainty used by the CART algorithm. Args: labels: The labels column of the dataset A measure of the probability that a label selected at random from the distribution of labels in a dataset would incorrectly label an example from that dataset. A Gini impurity of 0 indicates a perfectly certain dataset, that is one with a single unique label. The measure is calculated as 1 minus the sum of squared probabilities for each unique label. $I_{G}(p) = 1 - \\sum_{i=1}^{n}p_{i}^2$ for $\\mathnormal{n}$ classes with $i \\in \\{1, 2, ..., n\\}$ \"\"\" c , n = Counter ( labels ), len ( labels ) return 1 - sum (( count / n ) ** 2 for _elem , count in c . items ()) # type: ignore entropy ( labels ) Entropy, a measure of uncertainty used by the C.45 algorithm. Parameters: Name Type Description Default labels Union[List[Any], pd.Series] The labels column of the dataset required A measure of the amount of information and uncertainty realized in the outcome of drawing a value at random. An entropy of 0 indicates a dataset which can yield no new information, that is one with a single unique label. The measure is calculated as the negation of the sum of the products of the probability of drawing the label and the base two log of the same probability for each unique label. \\(I_{G}(p) = -\\sum_{i=1}^{n}p_{i} log_{2} p_{i}\\) for \\(\\mathnormal{n}\\) classes with \\(i \\in \\{1, 2, ..., n\\}\\) Source code in toolbox/algorithms/learning/decisiontree.py def entropy ( labels : Union [ List [ Any ], pd . Series ]) -> float : \"\"\"Entropy, a measure of uncertainty used by the C.45 algorithm. Args: labels: The labels column of the dataset A measure of the amount of information and uncertainty realized in the outcome of drawing a value at random. An entropy of 0 indicates a dataset which can yield no new information, that is one with a single unique label. The measure is calculated as the negation of the sum of the products of the probability of drawing the label and the base two log of the same probability for each unique label. $I_{G}(p) = -\\sum_{i=1}^{n}p_{i} log_{2} p_{i}$ for $\\mathnormal{n}$ classes with $i \\in \\{1, 2, ..., n\\}$ \"\"\" c , n = Counter ( labels ), len ( labels ) return - sum (( count / n ) * log2 (( count / n )) for _elem , count in c . items ())","title":"Decision-trees"},{"location":"machine-learning/decision-trees/#decision-trees","text":"","title":"Decision Trees"},{"location":"machine-learning/decision-trees/#toolbox.algorithms.learning.decisiontree.DecisionTreeClassifier","text":"Caveat This is a simple, pedagogical implementation of a decision-tree classifier. It is not likely to win you any Kaggle competitions on its own. It typically achieves accuracy scores around 0.7 and is slow to train. That said, this is what RandomForest, Ensemble methods, and XGBoost are based on, so it's well worth understanding how it works. Fitting the tree These classifiers work by recursively splitting a dataset using queries on the features in such a way as to maximize information gain at each new split. Information gain is calculated for each split by using an impurity function , which is sometimes known as an error function . Queries can be as complex as the implementer cares to make them, but in this implementation they are simple : all queries here are of the form \\(v \\ge w\\) . A value \\(w\\) is selected from the features and a candidate split is created : all examples for which the query yields True go to the left child and all others go to the right child. The candidate information gain is then evaluated. This process is repeated for each feature and for each unique feature value. The actual split is chosen to be the candidate with the highest information gain. For more on information gain, see the information_gain method's documentation. The leaves of the tree contain subsets of the dataset with no impurity, that is to say all examples of the subset belong to a single class. Making predictions By traveling through the tree with a novel, unlabeled example, applying the queries to its features' values to determine the path, we end up at a leaf node which determines the predicted class label for the novel example. Examples: >>> import pandas as pd >>> url = \"/\" . join ([ \"https://archive.ics.uci.edu\" , \"ml/machine-learning-databases\" , \"heart-disease/processed.cleveland.data\" ]) >>> df = pd . read_csv ( url , header = None , names = [ \"age\" , \"sex\" , \"cp\" , \"trestbps\" , \"chol\" , \"fbs\" , \"restecg\" , \"thalach\" , \"exang\" , \"oldpeak\" , \"slope\" , \"ca\" , \"thal\" , \"target\" ] ) >>> df [ 'target' ] . replace ({ 2 : 1 , 3 : 1 , 4 : 1 }, inplace = True ) >>> train = df [: 201 ] >>> test = df [ 201 :] >>> dtc = DecisionTreeClassifier ( train , 'target' , gini_impurity ) >>> dtc . fit () # 2 minute coffee break >>> _ = dtc . evaluate ( test ) MODEL Precision mean 0.688657 weighted 0.696078 Precision negative mean 0.688657 weighted 0.681236 Sensitivity mean 0.703326 weighted 0.701773 Specificity mean 0.703326 weighted 0.704880 Accuracy mean 0.696078 weighted 0.696078 F1-score mean 0.687395 weighted 0.690460 dtype: float64 CLASSES 0 1 True Positives 44.000000 27.000000 True Negatives 27.000000 44.000000 False Positives 10.000000 21.000000 False Negatives 21.000000 10.000000 Precision 0.814815 0.562500 Precision negative 0.562500 0.814815 Sensitivity 0.676923 0.729730 Specificity 0.729730 0.676923 Accuracy 0.696078 0.696078 F1-score 0.739496 0.635294 Weight (actual) 0.529412 0.470588 CONFUSION MATRIX 0 1 actual 0 44 10 1 21 27 A comparison The accuracy score for the given example is of 0.70, which is not far from scikit-learn's DecisionTreeClassifier accuracy of 0.76 given the same dataset. That said, with hyperparameter tuning (not possible here), the scikit-learn implementation can reach much higher accuracy scores (above 0.85).","title":"DecisionTreeClassifier"},{"location":"machine-learning/decision-trees/#toolbox.algorithms.learning.decisiontree.DecisionTreeClassifier.tree","text":"Pointer to the decision-tree's root node. The (binary) tree can be traversed by traveling through each node's left (True) and right (False) child nodes.","title":"tree"},{"location":"machine-learning/decision-trees/#toolbox.algorithms.learning.decisiontree.DecisionTreeClassifier.__init__","text":"Parameters: Name Type Description Default dataset pd.DataFrame Including one or more feature columns and a single labels column. required labels_col str The name of the labels column. required impurity_func Callable Options include gini_impurity and entropy , but any callable with the same signature which returns a float will do. required Source code in toolbox/algorithms/learning/decisiontree.py def __init__ ( self , dataset : pd . DataFrame , labels_col : str , impurity_func : Callable , ): \"\"\" Args: dataset: Including one or more feature columns and a single labels column. labels_col: The name of the labels column. impurity_func: Options include [gini_impurity] [toolbox.algorithms.learning.decisiontree.gini_impurity] and [entropy][toolbox.algorithms.learning.decisiontree.entropy], but any callable with the same signature which returns a float will do. \"\"\" self . features = dataset . drop ( labels_col , axis = 1 ) self . labels = dataset [ labels_col ] self . impurity_func = impurity_func self . _tree = None","title":"__init__()"},{"location":"machine-learning/decision-trees/#toolbox.algorithms.learning.decisiontree.DecisionTreeClassifier.evaluate","text":"Evaluate the performance of the decision-tree model. See evaluation.evaluate_classifier . Parameters: Name Type Description Default data pd.DataFrame A labeled DataFrame. The label column is expected to be named the same as the labels Series that was provided at initialization. required Source code in toolbox/algorithms/learning/decisiontree.py def evaluate ( self , data : pd . DataFrame , verbose = True , ) -> Tuple [ pd . Series , pd . DataFrame , pd . DataFrame ]: \"\"\"Evaluate the performance of the decision-tree model. See [evaluation.evaluate_classifier] [toolbox.algorithms.learning.evaluation.evaluate_classifier]. Args: data: A labeled DataFrame. The label column is expected to be named the same as the labels Series that was provided at initialization. \"\"\" predict_col = str ( self . labels . name ) actual_col = \"_\" . join ([ predict_col , \"actual\" ]) pred_df = self . predict ( data . drop ( predict_col , axis = 1 )) pred_df [ actual_col ] = data [ predict_col ] model_eval , class_eval , confusion_matrix = evaluate_classifier ( predict_df = pred_df , predict_col = predict_col , actual_col = actual_col , verbose = verbose , ) return ( model_eval , class_eval , confusion_matrix )","title":"evaluate()"},{"location":"machine-learning/decision-trees/#toolbox.algorithms.learning.decisiontree.DecisionTreeClassifier.find_best_split","text":"Find the split which yields the highest information gain. Parameters: Name Type Description Default idx Any The (pandas array) index of the examples of the node before the split. required Source code in toolbox/algorithms/learning/decisiontree.py def find_best_split ( self , idx : Any ): \"\"\"Find the split which yields the highest information gain. Args: idx (pd.array): The (pandas array) index of the examples of the node before the split. \"\"\" max_gain = 0.0 query = None true_idx = None false_idx = None for feature in self . features . columns : for value in self . features [ feature ] . unique (): split_condition = self . features . loc [ idx ][ feature ] . ge ( value ) n_true = split_condition . value_counts () . get ( True , 0 ) n_false = split_condition . value_counts () . get ( False , 0 ) if ( n_true == 0 ) or ( n_false == 0 ): continue gain = self . information_gain ( idx = idx , true_idx = split_condition ) if gain >= max_gain : true_idx = self . features . loc [ idx ][ split_condition ] . index false_idx = self . features . loc [ idx ][ ~ split_condition ] . index max_gain = gain query = DTCQuery ( feature , value , ge ) return ( max_gain , query , true_idx , false_idx )","title":"find_best_split()"},{"location":"machine-learning/decision-trees/#toolbox.algorithms.learning.decisiontree.DecisionTreeClassifier.fit","text":"Build the decision tree. Source code in toolbox/algorithms/learning/decisiontree.py def fit ( self ): \"\"\"Build the decision tree.\"\"\" def _fit ( node ): gain , query , left_idx , right_idx = self . find_best_split ( node . idx ) if gain == 0 : return DTCNode ( node . idx , leaf = True , label = self . labels . loc [ node . idx ] . values [ 0 ] ) node . query = query node . left = _fit ( DTCNode ( left_idx )) node . right = _fit ( DTCNode ( right_idx )) return node self . _tree = _fit ( DTCNode ( self . features . index ))","title":"fit()"},{"location":"machine-learning/decision-trees/#toolbox.algorithms.learning.decisiontree.DecisionTreeClassifier.information_gain","text":"Gain in information resulting from a split of the parent node. Weighted impurity is calculated for each side of the split, using the impurity function set at the initialization of the model, and the total impurity of the split is determined by the sum of these. The information gain is the difference of the impurity of the parent and this sum : \\(gain = I_{p} - w_{l}I_{l} + w_{r}I_{r}\\) The weights are calculated as the fraction of unique labels present in the split relative to the model's set of all known labels. Parameters: Name Type Description Default idx Any The (pandas array) index of the examples of the node before the split. required true_idx Any The (pandas array) index of the node's examples for which the query condition is true. ie.: The left child's index after the split. required Source code in toolbox/algorithms/learning/decisiontree.py def information_gain ( self , idx : Any , true_idx : Any ) -> float : \"\"\"Gain in information resulting from a split of the parent node. Weighted impurity is calculated for each side of the split, using the impurity function set at the initialization of the model, and the total impurity of the split is determined by the sum of these. The information gain is the difference of the impurity of the parent and this sum : $gain = I_{p} - w_{l}I_{l} + w_{r}I_{r}$ The weights are calculated as the fraction of unique labels present in the split relative to the model's set of all known labels. Args: idx (pd.array): The (pandas array) index of the examples of the node before the split. true_idx (pd.array): The (pandas array) index of the node's examples for which the query condition is true. ie.: The left child's index after the split. \"\"\" def _calc_impurity ( values : pd . Series , n_total : int , impurity_func : Callable , ) -> float : return len ( values ) / n_total * impurity_func ( values ) labels = self . labels . loc [ idx ] n_total = len ( labels ) true_impurity = _calc_impurity ( labels [ true_idx ], n_total , self . impurity_func ) false_impurity = _calc_impurity ( labels [ ~ true_idx ], n_total , self . impurity_func ) total_impurity = true_impurity + false_impurity return self . impurity_func ( labels ) - total_impurity","title":"information_gain()"},{"location":"machine-learning/decision-trees/#toolbox.algorithms.learning.decisiontree.DecisionTreeClassifier.predict","text":"Predict classes for a set of unlabeled examples. Parameters: Name Type Description Default data pd.DataFrame Unlabeled DataFrame of examples. required Source code in toolbox/algorithms/learning/decisiontree.py def predict ( self , data : pd . DataFrame ): \"\"\"Predict classes for a set of unlabeled examples. Args: data: Unlabeled DataFrame of examples. \"\"\" labels = list () for _ix , row in data . iterrows (): node = self . tree while not node . leaf : if node . query . operator ( row [ node . query . feature ], node . query . value ): node = node . left else : node = node . right labels . append ( node . label ) data [ self . labels . name ] = labels return data","title":"predict()"},{"location":"machine-learning/decision-trees/#toolbox.algorithms.learning.decisiontree.gini_impurity","text":"Gini impurity, a measure of uncertainty used by the CART algorithm. Parameters: Name Type Description Default labels Union[List[Any], pd.Series] The labels column of the dataset required A measure of the probability that a label selected at random from the distribution of labels in a dataset would incorrectly label an example from that dataset. A Gini impurity of 0 indicates a perfectly certain dataset, that is one with a single unique label. The measure is calculated as 1 minus the sum of squared probabilities for each unique label. \\(I_{G}(p) = 1 - \\sum_{i=1}^{n}p_{i}^2\\) for \\(\\mathnormal{n}\\) classes with \\(i \\in \\{1, 2, ..., n\\}\\) Source code in toolbox/algorithms/learning/decisiontree.py def gini_impurity ( labels : Union [ List [ Any ], pd . Series ]) -> float : \"\"\"Gini impurity, a measure of uncertainty used by the CART algorithm. Args: labels: The labels column of the dataset A measure of the probability that a label selected at random from the distribution of labels in a dataset would incorrectly label an example from that dataset. A Gini impurity of 0 indicates a perfectly certain dataset, that is one with a single unique label. The measure is calculated as 1 minus the sum of squared probabilities for each unique label. $I_{G}(p) = 1 - \\sum_{i=1}^{n}p_{i}^2$ for $\\mathnormal{n}$ classes with $i \\in \\{1, 2, ..., n\\}$ \"\"\" c , n = Counter ( labels ), len ( labels ) return 1 - sum (( count / n ) ** 2 for _elem , count in c . items ()) # type: ignore","title":"gini_impurity()"},{"location":"machine-learning/decision-trees/#toolbox.algorithms.learning.decisiontree.entropy","text":"Entropy, a measure of uncertainty used by the C.45 algorithm. Parameters: Name Type Description Default labels Union[List[Any], pd.Series] The labels column of the dataset required A measure of the amount of information and uncertainty realized in the outcome of drawing a value at random. An entropy of 0 indicates a dataset which can yield no new information, that is one with a single unique label. The measure is calculated as the negation of the sum of the products of the probability of drawing the label and the base two log of the same probability for each unique label. \\(I_{G}(p) = -\\sum_{i=1}^{n}p_{i} log_{2} p_{i}\\) for \\(\\mathnormal{n}\\) classes with \\(i \\in \\{1, 2, ..., n\\}\\) Source code in toolbox/algorithms/learning/decisiontree.py def entropy ( labels : Union [ List [ Any ], pd . Series ]) -> float : \"\"\"Entropy, a measure of uncertainty used by the C.45 algorithm. Args: labels: The labels column of the dataset A measure of the amount of information and uncertainty realized in the outcome of drawing a value at random. An entropy of 0 indicates a dataset which can yield no new information, that is one with a single unique label. The measure is calculated as the negation of the sum of the products of the probability of drawing the label and the base two log of the same probability for each unique label. $I_{G}(p) = -\\sum_{i=1}^{n}p_{i} log_{2} p_{i}$ for $\\mathnormal{n}$ classes with $i \\in \\{1, 2, ..., n\\}$ \"\"\" c , n = Counter ( labels ), len ( labels ) return - sum (( count / n ) * log2 (( count / n )) for _elem , count in c . items ())","title":"entropy()"},{"location":"machine-learning/evaluation/","text":"Evaluation Classification evaluate_classifier ( predict_df , predict_col , actual_col , verbose = True ) Evaluates a classification model overall and for each of the its classes. It is capable of evaluating both binary and multi-class classifiers. This function wraps: evaluate_classifier_model evaluate_classifier_per_class generate_confusion_matrix Parameters: Name Type Description Default predict_df DataFrame A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. required predict_col str The name of the column holding the predicted classes. required actual_col str The name of the column holding the actual classes. required verbose bool Whether or not to print out the results. True Returns: Type Description Tuple[pandas.core.series.Series, pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] A tuple containing: model_eval class_eval confusion_matrix Examples: MODEL Precision mean 0.657538 weighted 0.666667 Precision negative mean 0.657538 weighted 0.648410 Sensitivity mean 0.657538 weighted 0.666667 Specificity mean 0.657538 weighted 0.648410 Accuracy mean 0.666667 weighted 0.666667 F1-score mean 0.657538 weighted 0.666667 CLASSES No Yes True Positives 122.000000 74.000000 True Negatives 74.000000 122.000000 False Positives 49.000000 49.000000 False Negatives 49.000000 49.000000 Precision 0.713450 0.601626 Precision negative 0.601626 0.713450 Sensitivity 0.713450 0.601626 Specificity 0.601626 0.713450 Accuracy 0.666667 0.666667 F1-score 0.713450 0.601626 Weight (actual) 0.581633 0.418367 CONFUSION MATRIX No Yes actual No 122 49 Yes 49 74 Source code in toolbox/algorithms/learning/evaluation.py def evaluate_classifier ( predict_df : pd . DataFrame , predict_col : str , actual_col : str , verbose : bool = True , ) -> Tuple [ pd . Series , pd . DataFrame , pd . DataFrame ]: \"\"\" Evaluates a classification model overall and for each of the its classes. It is capable of evaluating both binary and multi-class classifiers. This function wraps: - [evaluate_classifier_model] [toolbox.algorithms.learning.evaluation.evaluate_classifier_model] - [evaluate_classifier_per_class] [toolbox.algorithms.learning.evaluation.evaluate_classifier_per_class] - [generate_confusion_matrix] [toolbox.algorithms.learning.evaluation.generate_confusion_matrix] Args: predict_df: A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. predict_col: The name of the column holding the predicted classes. actual_col: The name of the column holding the actual classes. verbose: Whether or not to print out the results. Returns: A tuple containing: - `model_eval` - `class_eval` - `confusion_matrix` Examples: ``` MODEL Precision mean 0.657538 weighted 0.666667 Precision negative mean 0.657538 weighted 0.648410 Sensitivity mean 0.657538 weighted 0.666667 Specificity mean 0.657538 weighted 0.648410 Accuracy mean 0.666667 weighted 0.666667 F1-score mean 0.657538 weighted 0.666667 CLASSES No Yes True Positives 122.000000 74.000000 True Negatives 74.000000 122.000000 False Positives 49.000000 49.000000 False Negatives 49.000000 49.000000 Precision 0.713450 0.601626 Precision negative 0.601626 0.713450 Sensitivity 0.713450 0.601626 Specificity 0.601626 0.713450 Accuracy 0.666667 0.666667 F1-score 0.713450 0.601626 Weight (actual) 0.581633 0.418367 CONFUSION MATRIX No Yes actual No 122 49 Yes 49 74 ``` \"\"\" classes = set ( predict_df [ predict_col ] . values ) . union ( set ( predict_df [ actual_col ] . values ) ) confusion_matrix = generate_confusion_matrix ( predict_df = predict_df , classes = classes , predict_col = predict_col , actual_col = actual_col , ) class_eval = evaluate_classifier_per_class ( predict_df = predict_df , confusion_matrix = confusion_matrix , classes = classes , actual_col = actual_col , ) model_eval = evaluate_classifier_model ( class_eval ) if verbose : print ( \" \\n MODEL \\n \" ) print ( model_eval ) print ( \" \\n CLASSES \\n \" ) print ( class_eval ) print ( \" \\n CONFUSION MATRIX \\n \" ) print ( confusion_matrix , \" \\n \" ) return ( model_eval , class_eval , confusion_matrix ) evaluate_classifier_model ( class_eval_df ) Evaluates all (see evaluate_all ) statistical measures, both mean across and weighted by class. Parameters: Name Type Description Default class_eval_df DataFrame Classifier evaluation per class, as generated by evaluate_classifier_per_class required Returns: Type Description Series With a multi-index, of which the 0 level is the statistical measure and the 1 level is the method. Examples: Precision mean 0.657538 weighted 0.666667 Precision negative mean 0.657538 weighted 0.648410 Sensitivity mean 0.657538 weighted 0.666667 Specificity mean 0.657538 weighted 0.648410 Accuracy mean 0.666667 weighted 0.666667 F1-score mean 0.657538 weighted 0.666667 Source code in toolbox/algorithms/learning/evaluation.py def evaluate_classifier_model ( class_eval_df : pd . DataFrame ) -> pd . Series : \"\"\" Evaluates all (see [evaluate_all] [toolbox.algorithms.learning.evaluation.evaluate_all]) statistical measures, both *mean* across and *weighted* by class. Args: class_eval_df: Classifier evaluation per class, as generated by [evaluate_classifier_per_class] [toolbox.algorithms.learning.evaluation.evaluate_classifier_per_class] Returns: With a multi-index, of which the 0 level is the statistical measure and the 1 level is the method. Examples: ``` Precision mean 0.657538 weighted 0.666667 Precision negative mean 0.657538 weighted 0.648410 Sensitivity mean 0.657538 weighted 0.666667 Specificity mean 0.657538 weighted 0.648410 Accuracy mean 0.666667 weighted 0.666667 F1-score mean 0.657538 weighted 0.666667 ``` \"\"\" weights = class_eval_df . loc [ \"Weight (actual)\" ] eval_dict = { ( \"Precision\" , \"mean\" ): class_eval_df . loc [ \"Precision\" ] . mean (), ( \"Precision\" , \"weighted\" ): ( class_eval_df . loc [ \"Precision\" ] * weights ) . sum (), ( \"Precision negative\" , \"mean\" ): class_eval_df . loc [ \"Precision negative\" ] . mean (), ( \"Precision negative\" , \"weighted\" ): ( class_eval_df . loc [ \"Precision negative\" ] * weights ) . sum (), ( \"Sensitivity\" , \"mean\" ): class_eval_df . loc [ \"Sensitivity\" ] . mean (), ( \"Sensitivity\" , \"weighted\" ): ( class_eval_df . loc [ \"Sensitivity\" ] * weights ) . sum (), ( \"Specificity\" , \"mean\" ): class_eval_df . loc [ \"Specificity\" ] . mean (), ( \"Specificity\" , \"weighted\" ): ( class_eval_df . loc [ \"Specificity\" ] * weights ) . sum (), ( \"Accuracy\" , \"mean\" ): class_eval_df . loc [ \"Accuracy\" ] . mean (), ( \"Accuracy\" , \"weighted\" ): ( class_eval_df . loc [ \"Accuracy\" ] * weights ) . sum (), ( \"F1-score\" , \"mean\" ): class_eval_df . loc [ \"F1-score\" ] . mean (), ( \"F1-score\" , \"weighted\" ): ( class_eval_df . loc [ \"F1-score\" ] * weights ) . sum (), } return pd . Series ( eval_dict ) evaluate_classifier_per_class ( predict_df , confusion_matrix , classes , actual_col ) Evaluates all (see evaluate_all ) statistical measures, the classification outcomes (see get_classification_outcomes ), and the relative weights for each class label. Parameters: Name Type Description Default predict_df DataFrame A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. required confusion_matrix DataFrame The result of calling generate_confusion_matrix required classes Set[Any] The set of all class labels required actual_col str The name of the column holding the actual classes. required Returns: Type Description DataFrame With the statistical measure names determining the index and a column per class label. Examples: No Yes True Positives 122.000000 74.000000 True Negatives 74.000000 122.000000 False Positives 49.000000 49.000000 False Negatives 49.000000 49.000000 Precision 0.713450 0.601626 Precision negative 0.601626 0.713450 Sensitivity 0.713450 0.601626 Specificity 0.601626 0.713450 Accuracy 0.666667 0.666667 F1-score 0.713450 0.601626 Weight (actual) 0.581633 0.418367 Source code in toolbox/algorithms/learning/evaluation.py def evaluate_classifier_per_class ( predict_df : pd . DataFrame , confusion_matrix : pd . DataFrame , classes : Set [ Any ], actual_col : str , ) -> pd . DataFrame : \"\"\" Evaluates all (see [evaluate_all] [toolbox.algorithms.learning.evaluation.evaluate_all]) statistical measures, the classification outcomes (see [get_classification_outcomes] [toolbox.algorithms.learning.evaluation.get_classification_outcomes]), and the relative weights for each class label. Args: predict_df: A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. confusion_matrix: The result of calling [generate_confusion_matrix] [toolbox.algorithms.learning.evaluation.generate_confusion_matrix] classes: The set of all class labels actual_col: The name of the column holding the actual classes. Returns: With the statistical measure names determining the index and a column per class label. Examples: ``` No Yes True Positives 122.000000 74.000000 True Negatives 74.000000 122.000000 False Positives 49.000000 49.000000 False Negatives 49.000000 49.000000 Precision 0.713450 0.601626 Precision negative 0.601626 0.713450 Sensitivity 0.713450 0.601626 Specificity 0.601626 0.713450 Accuracy 0.666667 0.666667 F1-score 0.713450 0.601626 Weight (actual) 0.581633 0.418367 ``` \"\"\" weights = predict_df [ actual_col ] . value_counts ( normalize = True ) class_eval : dict = {} for class_name in classes : class_eval . update ({ class_name : {}}) tp , tn , fp , fn = get_classification_outcomes ( confusion_matrix = confusion_matrix , classes = classes , class_name = class_name , ) eval_dict = evaluate_all ( tp , tn , fp , fn ) eval_dict . update ({ \"Weight (actual)\" : weights . get ( class_name , 0 )}) class_eval [ class_name ] . update ( eval_dict ) return pd . DataFrame ( class_eval ) . fillna ( 0 ) generate_confusion_matrix ( predict_df , classes , predict_col , actual_col ) The confusion matrix summarises how well a model labels examples belonging to a set of classes. In the case of binary classification - where there are two classes - the confusion matrix is a \\(2*2\\) matrix. More generally: for \\(n\\) classes the matrix will have a shape of \\(n*n\\) . Here, the predicted labels determine the column headers and the actual labels determine the row headers. The matrix is used to calculate the classification outcomes. See get_classification_outcomes Parameters: Name Type Description Default predict_df DataFrame A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. required classes Set[Any] The set of unique class labels, being the union of the unique actual and predicted class labels. required predict_col str The name of the column holding the predicted classes. required actual_col str The name of the column holding the actual classes. required Returns: Type Description DataFrame For \\(n\\) classes, an \\(n*n\\) pd.DataFrame with predicted labels determining the column headers and the actual labels determining the row headers. headers Examples: Given a simple binary classifier with two classes 'Yes' and 'No', the following is a possible result: No Yes actual No 122 49 Yes 49 74 Source code in toolbox/algorithms/learning/evaluation.py def generate_confusion_matrix ( predict_df : pd . DataFrame , classes : Set [ Any ], predict_col : str , actual_col : str , ) -> pd . DataFrame : \"\"\" The confusion matrix summarises how well a model labels examples belonging to a set of classes. In the case of binary classification - where there are two classes - the confusion matrix is a $2*2$ matrix. More generally: for $n$ classes the matrix will have a shape of $n*n$. Here, the *predicted* labels determine the column headers and the *actual* labels determine the row headers. The matrix is used to calculate the classification outcomes. See [get_classification_outcomes] [toolbox.algorithms.learning.evaluation.get_classification_outcomes] Args: predict_df: A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. classes: The set of unique class labels, being the union of the unique actual and predicted class labels. predict_col: The name of the column holding the predicted classes. actual_col: The name of the column holding the actual classes. Returns: For $n$ classes, an $n*n$ pd.DataFrame with *predicted* labels determining the column headers and the *actual* labels determining the row headers. headers Examples: Given a simple binary classifier with two classes 'Yes' and 'No', the following is a possible result: ``` No Yes actual No 122 49 Yes 49 74 ``` \"\"\" confusion_matrix = pd . DataFrame ( 0 , index = classes , columns = classes ) for _ix , pred , actual in predict_df [[ predict_col , actual_col ]] . itertuples (): confusion_matrix . loc [ actual ][ pred ] += 1 confusion_matrix . index . name = \"actual\" return confusion_matrix get_classification_outcomes ( confusion_matrix , classes , class_name ) Given a confusion matrix, this function counts the cases of: True Positives : classifications that accurately labeled a class True Negatives : classifications that accurately labeled an example as not belonging to a class. False Positives : classifications that attributed the wrong label to an example. False Negatives : classifications that falsely claimed that an example does not belong to a class. Parameters: Name Type Description Default confusion_matrix DataFrame The result of calling generate_confusion_matrix required classes Set[Any] The set of all class labels required class_name str The name (label) of the class being evaluated. required Returns: Type Description Tuple[int, int, int, int] tp : Count of True Positives tn : Count of True Negatives fp : Count of False Positives fn : Count of False Negatives Source code in toolbox/algorithms/learning/evaluation.py def get_classification_outcomes ( confusion_matrix : pd . DataFrame , classes : Set [ Any ], class_name : str , ) -> Tuple [ int , int , int , int ]: \"\"\" Given a confusion matrix, this function counts the cases of: - **True Positives** : classifications that accurately labeled a class - **True Negatives** : classifications that accurately labeled an example as not belonging to a class. - **False Positives** : classifications that attributed the wrong label to an example. - **False Negatives** : classifications that falsely claimed that an example does not belong to a class. Args: confusion_matrix: The result of calling [generate_confusion_matrix] [toolbox.algorithms.learning.evaluation.generate_confusion_matrix] classes: The set of all class labels class_name: The name (label) of the class being evaluated. Returns: - `tp`: Count of True Positives - `tn`: Count of True Negatives - `fp`: Count of False Positives - `fn`: Count of False Negatives \"\"\" excl_idx = classes . difference ( set (( class_name ,))) tp = confusion_matrix . loc [ class_name , class_name ] tn = confusion_matrix . loc [ excl_idx , excl_idx ] . sum () . sum () fp = confusion_matrix . loc [ class_name , excl_idx ] . sum () fn = confusion_matrix . loc [ excl_idx , class_name ] . sum () return ( tp , tn , fp , fn ) evaluate_precision ( tp , fp ) Precision, aka Positive Predictive Value (PPV). \\(PPV=\\dfrac{TP}{TP + FP}\\) Parameters: Name Type Description Default tp int True Positives required fp int False Positives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_precision ( tp : int , fp : int ) -> float : \"\"\"Precision, aka Positive Predictive Value (PPV). $PPV=\\dfrac{TP}{TP + FP}$ Args: tp: True Positives fp: False Positives \"\"\" try : return tp / ( tp + fp ) except ZeroDivisionError : return 0.0 evaluate_precision_neg ( tn , fn ) Negative precision, aka Negative Predictive Value (NPV). \\(NPV=\\dfrac{TN}{TN + FN}\\) Parameters: Name Type Description Default tn int True Negatives required fn int False Negatives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_precision_neg ( tn : int , fn : int ) -> float : \"\"\"Negative precision, aka Negative Predictive Value (NPV). $NPV=\\dfrac{TN}{TN + FN}$ Args: tn: True Negatives fn: False Negatives \"\"\" try : return tn / ( tn + fn ) except ZeroDivisionError : return 0.0 evaluate_sensitivity ( tp , fn ) Sensitivity, aka Recall, aka True Positive Rate (TPR). \\(TPR=\\dfrac{TP}{TP + FN}\\) Parameters: Name Type Description Default tp int True Positives required fn int False Negatives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_sensitivity ( tp : int , fn : int ) -> float : \"\"\"Sensitivity, aka Recall, aka True Positive Rate (TPR). $TPR=\\dfrac{TP}{TP + FN}$ Args: tp: True Positives fn: False Negatives \"\"\" try : return tp / ( tp + fn ) except ZeroDivisionError : return 0.0 evaluate_specificity ( tn , fp ) Specificity, aka True Negative Rate (TNR). \\(TNR=\\dfrac{TP}{TP + FP}\\) Parameters: Name Type Description Default tp True Positives required fp int False Positives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_specificity ( tn : int , fp : int ) -> float : \"\"\"Specificity, aka True Negative Rate (TNR). $TNR=\\dfrac{TP}{TP + FP}$ Args: tp: True Positives fp: False Positives \"\"\" try : return tn / ( tn + fp ) except ZeroDivisionError : return 0.0 evaluate_accuracy ( tp , tn , fp , fn ) Accuracy (ACC). \\(ACC=\\dfrac{TP + TN}{TP + TN + FP + FN}\\) Parameters: Name Type Description Default tp int True Positives required tn int True Negatives required fp int False Positives required fn int False Negatives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_accuracy ( tp : int , tn : int , fp : int , fn : int ) -> float : \"\"\"Accuracy (ACC). $ACC=\\dfrac{TP + TN}{TP + TN + FP + FN}$ Args: tp: True Positives tn: True Negatives fp: False Positives fn: False Negatives \"\"\" try : return ( tp + tn ) / ( tp + tn + fp + fn ) except ZeroDivisionError : return 0.0 evaluate_f1 ( tp , fp , fn ) F1-score. F1-score \\(=\\dfrac{2TP}{2TP + FP + FN}\\) Parameters: Name Type Description Default tp int True Positives required fp int False Positives required fn int False Negatives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_f1 ( tp : int , fp : int , fn : int ) -> float : \"\"\"F1-score. *F1-score* $=\\dfrac{2TP}{2TP + FP + FN}$ Args: tp: True Positives fp: False Positives fn: False Negatives \"\"\" try : return 2 * tp / ( 2 * tp + fp + fn ) except ZeroDivisionError : return 0.0 evaluate_all ( tp , tn , fp , fn ) Evaluates the following statistical measures: precision negative precision sensitivity (recall) specificity accuracy f1-score Parameters: Name Type Description Default tp int True Positives required tn int True Negatives required fp int False Positives required fn int False Negatives required Returns: Type Description dict See source for dictionary keys. Source code in toolbox/algorithms/learning/evaluation.py def evaluate_all ( tp : int , tn : int , fp : int , fn : int ) -> dict [ str , float ]: \"\"\" Evaluates the following statistical measures: - [precision] [toolbox.algorithms.learning.evaluation.evaluate_precision] - [negative precision] [toolbox.algorithms.learning.evaluation.evaluate_precision_neg] - [sensitivity (recall)] [toolbox.algorithms.learning.evaluation.evaluate_sensitivity] - [specificity] [toolbox.algorithms.learning.evaluation.evaluate_specificity] - [accuracy] [toolbox.algorithms.learning.evaluation.evaluate_accuracy] - [f1-score] [toolbox.algorithms.learning.evaluation.evaluate_f1] Args: tp: True Positives tn: True Negatives fp: False Positives fn: False Negatives Returns: See source for dictionary keys. \"\"\" return { \"True Positives\" : tp , \"True Negatives\" : tn , \"False Positives\" : fp , \"False Negatives\" : fn , \"Precision\" : evaluate_precision ( tp , fp ), \"Precision negative\" : evaluate_precision_neg ( tn , fn ), \"Sensitivity\" : evaluate_sensitivity ( tp , fn ), \"Specificity\" : evaluate_specificity ( tn , fp ), \"Accuracy\" : evaluate_accuracy ( tp , tn , fp , fn ), \"F1-score\" : evaluate_f1 ( tp , fp , fn ), }","title":"Evaluation"},{"location":"machine-learning/evaluation/#evaluation","text":"","title":"Evaluation"},{"location":"machine-learning/evaluation/#classification","text":"","title":"Classification"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.evaluate_classifier","text":"Evaluates a classification model overall and for each of the its classes. It is capable of evaluating both binary and multi-class classifiers. This function wraps: evaluate_classifier_model evaluate_classifier_per_class generate_confusion_matrix Parameters: Name Type Description Default predict_df DataFrame A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. required predict_col str The name of the column holding the predicted classes. required actual_col str The name of the column holding the actual classes. required verbose bool Whether or not to print out the results. True Returns: Type Description Tuple[pandas.core.series.Series, pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] A tuple containing: model_eval class_eval confusion_matrix Examples: MODEL Precision mean 0.657538 weighted 0.666667 Precision negative mean 0.657538 weighted 0.648410 Sensitivity mean 0.657538 weighted 0.666667 Specificity mean 0.657538 weighted 0.648410 Accuracy mean 0.666667 weighted 0.666667 F1-score mean 0.657538 weighted 0.666667 CLASSES No Yes True Positives 122.000000 74.000000 True Negatives 74.000000 122.000000 False Positives 49.000000 49.000000 False Negatives 49.000000 49.000000 Precision 0.713450 0.601626 Precision negative 0.601626 0.713450 Sensitivity 0.713450 0.601626 Specificity 0.601626 0.713450 Accuracy 0.666667 0.666667 F1-score 0.713450 0.601626 Weight (actual) 0.581633 0.418367 CONFUSION MATRIX No Yes actual No 122 49 Yes 49 74 Source code in toolbox/algorithms/learning/evaluation.py def evaluate_classifier ( predict_df : pd . DataFrame , predict_col : str , actual_col : str , verbose : bool = True , ) -> Tuple [ pd . Series , pd . DataFrame , pd . DataFrame ]: \"\"\" Evaluates a classification model overall and for each of the its classes. It is capable of evaluating both binary and multi-class classifiers. This function wraps: - [evaluate_classifier_model] [toolbox.algorithms.learning.evaluation.evaluate_classifier_model] - [evaluate_classifier_per_class] [toolbox.algorithms.learning.evaluation.evaluate_classifier_per_class] - [generate_confusion_matrix] [toolbox.algorithms.learning.evaluation.generate_confusion_matrix] Args: predict_df: A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. predict_col: The name of the column holding the predicted classes. actual_col: The name of the column holding the actual classes. verbose: Whether or not to print out the results. Returns: A tuple containing: - `model_eval` - `class_eval` - `confusion_matrix` Examples: ``` MODEL Precision mean 0.657538 weighted 0.666667 Precision negative mean 0.657538 weighted 0.648410 Sensitivity mean 0.657538 weighted 0.666667 Specificity mean 0.657538 weighted 0.648410 Accuracy mean 0.666667 weighted 0.666667 F1-score mean 0.657538 weighted 0.666667 CLASSES No Yes True Positives 122.000000 74.000000 True Negatives 74.000000 122.000000 False Positives 49.000000 49.000000 False Negatives 49.000000 49.000000 Precision 0.713450 0.601626 Precision negative 0.601626 0.713450 Sensitivity 0.713450 0.601626 Specificity 0.601626 0.713450 Accuracy 0.666667 0.666667 F1-score 0.713450 0.601626 Weight (actual) 0.581633 0.418367 CONFUSION MATRIX No Yes actual No 122 49 Yes 49 74 ``` \"\"\" classes = set ( predict_df [ predict_col ] . values ) . union ( set ( predict_df [ actual_col ] . values ) ) confusion_matrix = generate_confusion_matrix ( predict_df = predict_df , classes = classes , predict_col = predict_col , actual_col = actual_col , ) class_eval = evaluate_classifier_per_class ( predict_df = predict_df , confusion_matrix = confusion_matrix , classes = classes , actual_col = actual_col , ) model_eval = evaluate_classifier_model ( class_eval ) if verbose : print ( \" \\n MODEL \\n \" ) print ( model_eval ) print ( \" \\n CLASSES \\n \" ) print ( class_eval ) print ( \" \\n CONFUSION MATRIX \\n \" ) print ( confusion_matrix , \" \\n \" ) return ( model_eval , class_eval , confusion_matrix )","title":"evaluate_classifier()"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.evaluate_classifier_model","text":"Evaluates all (see evaluate_all ) statistical measures, both mean across and weighted by class. Parameters: Name Type Description Default class_eval_df DataFrame Classifier evaluation per class, as generated by evaluate_classifier_per_class required Returns: Type Description Series With a multi-index, of which the 0 level is the statistical measure and the 1 level is the method. Examples: Precision mean 0.657538 weighted 0.666667 Precision negative mean 0.657538 weighted 0.648410 Sensitivity mean 0.657538 weighted 0.666667 Specificity mean 0.657538 weighted 0.648410 Accuracy mean 0.666667 weighted 0.666667 F1-score mean 0.657538 weighted 0.666667 Source code in toolbox/algorithms/learning/evaluation.py def evaluate_classifier_model ( class_eval_df : pd . DataFrame ) -> pd . Series : \"\"\" Evaluates all (see [evaluate_all] [toolbox.algorithms.learning.evaluation.evaluate_all]) statistical measures, both *mean* across and *weighted* by class. Args: class_eval_df: Classifier evaluation per class, as generated by [evaluate_classifier_per_class] [toolbox.algorithms.learning.evaluation.evaluate_classifier_per_class] Returns: With a multi-index, of which the 0 level is the statistical measure and the 1 level is the method. Examples: ``` Precision mean 0.657538 weighted 0.666667 Precision negative mean 0.657538 weighted 0.648410 Sensitivity mean 0.657538 weighted 0.666667 Specificity mean 0.657538 weighted 0.648410 Accuracy mean 0.666667 weighted 0.666667 F1-score mean 0.657538 weighted 0.666667 ``` \"\"\" weights = class_eval_df . loc [ \"Weight (actual)\" ] eval_dict = { ( \"Precision\" , \"mean\" ): class_eval_df . loc [ \"Precision\" ] . mean (), ( \"Precision\" , \"weighted\" ): ( class_eval_df . loc [ \"Precision\" ] * weights ) . sum (), ( \"Precision negative\" , \"mean\" ): class_eval_df . loc [ \"Precision negative\" ] . mean (), ( \"Precision negative\" , \"weighted\" ): ( class_eval_df . loc [ \"Precision negative\" ] * weights ) . sum (), ( \"Sensitivity\" , \"mean\" ): class_eval_df . loc [ \"Sensitivity\" ] . mean (), ( \"Sensitivity\" , \"weighted\" ): ( class_eval_df . loc [ \"Sensitivity\" ] * weights ) . sum (), ( \"Specificity\" , \"mean\" ): class_eval_df . loc [ \"Specificity\" ] . mean (), ( \"Specificity\" , \"weighted\" ): ( class_eval_df . loc [ \"Specificity\" ] * weights ) . sum (), ( \"Accuracy\" , \"mean\" ): class_eval_df . loc [ \"Accuracy\" ] . mean (), ( \"Accuracy\" , \"weighted\" ): ( class_eval_df . loc [ \"Accuracy\" ] * weights ) . sum (), ( \"F1-score\" , \"mean\" ): class_eval_df . loc [ \"F1-score\" ] . mean (), ( \"F1-score\" , \"weighted\" ): ( class_eval_df . loc [ \"F1-score\" ] * weights ) . sum (), } return pd . Series ( eval_dict )","title":"evaluate_classifier_model()"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.evaluate_classifier_per_class","text":"Evaluates all (see evaluate_all ) statistical measures, the classification outcomes (see get_classification_outcomes ), and the relative weights for each class label. Parameters: Name Type Description Default predict_df DataFrame A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. required confusion_matrix DataFrame The result of calling generate_confusion_matrix required classes Set[Any] The set of all class labels required actual_col str The name of the column holding the actual classes. required Returns: Type Description DataFrame With the statistical measure names determining the index and a column per class label. Examples: No Yes True Positives 122.000000 74.000000 True Negatives 74.000000 122.000000 False Positives 49.000000 49.000000 False Negatives 49.000000 49.000000 Precision 0.713450 0.601626 Precision negative 0.601626 0.713450 Sensitivity 0.713450 0.601626 Specificity 0.601626 0.713450 Accuracy 0.666667 0.666667 F1-score 0.713450 0.601626 Weight (actual) 0.581633 0.418367 Source code in toolbox/algorithms/learning/evaluation.py def evaluate_classifier_per_class ( predict_df : pd . DataFrame , confusion_matrix : pd . DataFrame , classes : Set [ Any ], actual_col : str , ) -> pd . DataFrame : \"\"\" Evaluates all (see [evaluate_all] [toolbox.algorithms.learning.evaluation.evaluate_all]) statistical measures, the classification outcomes (see [get_classification_outcomes] [toolbox.algorithms.learning.evaluation.get_classification_outcomes]), and the relative weights for each class label. Args: predict_df: A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. confusion_matrix: The result of calling [generate_confusion_matrix] [toolbox.algorithms.learning.evaluation.generate_confusion_matrix] classes: The set of all class labels actual_col: The name of the column holding the actual classes. Returns: With the statistical measure names determining the index and a column per class label. Examples: ``` No Yes True Positives 122.000000 74.000000 True Negatives 74.000000 122.000000 False Positives 49.000000 49.000000 False Negatives 49.000000 49.000000 Precision 0.713450 0.601626 Precision negative 0.601626 0.713450 Sensitivity 0.713450 0.601626 Specificity 0.601626 0.713450 Accuracy 0.666667 0.666667 F1-score 0.713450 0.601626 Weight (actual) 0.581633 0.418367 ``` \"\"\" weights = predict_df [ actual_col ] . value_counts ( normalize = True ) class_eval : dict = {} for class_name in classes : class_eval . update ({ class_name : {}}) tp , tn , fp , fn = get_classification_outcomes ( confusion_matrix = confusion_matrix , classes = classes , class_name = class_name , ) eval_dict = evaluate_all ( tp , tn , fp , fn ) eval_dict . update ({ \"Weight (actual)\" : weights . get ( class_name , 0 )}) class_eval [ class_name ] . update ( eval_dict ) return pd . DataFrame ( class_eval ) . fillna ( 0 )","title":"evaluate_classifier_per_class()"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.generate_confusion_matrix","text":"The confusion matrix summarises how well a model labels examples belonging to a set of classes. In the case of binary classification - where there are two classes - the confusion matrix is a \\(2*2\\) matrix. More generally: for \\(n\\) classes the matrix will have a shape of \\(n*n\\) . Here, the predicted labels determine the column headers and the actual labels determine the row headers. The matrix is used to calculate the classification outcomes. See get_classification_outcomes Parameters: Name Type Description Default predict_df DataFrame A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. required classes Set[Any] The set of unique class labels, being the union of the unique actual and predicted class labels. required predict_col str The name of the column holding the predicted classes. required actual_col str The name of the column holding the actual classes. required Returns: Type Description DataFrame For \\(n\\) classes, an \\(n*n\\) pd.DataFrame with predicted labels determining the column headers and the actual labels determining the row headers. headers Examples: Given a simple binary classifier with two classes 'Yes' and 'No', the following is a possible result: No Yes actual No 122 49 Yes 49 74 Source code in toolbox/algorithms/learning/evaluation.py def generate_confusion_matrix ( predict_df : pd . DataFrame , classes : Set [ Any ], predict_col : str , actual_col : str , ) -> pd . DataFrame : \"\"\" The confusion matrix summarises how well a model labels examples belonging to a set of classes. In the case of binary classification - where there are two classes - the confusion matrix is a $2*2$ matrix. More generally: for $n$ classes the matrix will have a shape of $n*n$. Here, the *predicted* labels determine the column headers and the *actual* labels determine the row headers. The matrix is used to calculate the classification outcomes. See [get_classification_outcomes] [toolbox.algorithms.learning.evaluation.get_classification_outcomes] Args: predict_df: A DataFrame containing a set of features, their classes as predicted by the classification model, and the actual classes correctly labeling each example. classes: The set of unique class labels, being the union of the unique actual and predicted class labels. predict_col: The name of the column holding the predicted classes. actual_col: The name of the column holding the actual classes. Returns: For $n$ classes, an $n*n$ pd.DataFrame with *predicted* labels determining the column headers and the *actual* labels determining the row headers. headers Examples: Given a simple binary classifier with two classes 'Yes' and 'No', the following is a possible result: ``` No Yes actual No 122 49 Yes 49 74 ``` \"\"\" confusion_matrix = pd . DataFrame ( 0 , index = classes , columns = classes ) for _ix , pred , actual in predict_df [[ predict_col , actual_col ]] . itertuples (): confusion_matrix . loc [ actual ][ pred ] += 1 confusion_matrix . index . name = \"actual\" return confusion_matrix","title":"generate_confusion_matrix()"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.get_classification_outcomes","text":"Given a confusion matrix, this function counts the cases of: True Positives : classifications that accurately labeled a class True Negatives : classifications that accurately labeled an example as not belonging to a class. False Positives : classifications that attributed the wrong label to an example. False Negatives : classifications that falsely claimed that an example does not belong to a class. Parameters: Name Type Description Default confusion_matrix DataFrame The result of calling generate_confusion_matrix required classes Set[Any] The set of all class labels required class_name str The name (label) of the class being evaluated. required Returns: Type Description Tuple[int, int, int, int] tp : Count of True Positives tn : Count of True Negatives fp : Count of False Positives fn : Count of False Negatives Source code in toolbox/algorithms/learning/evaluation.py def get_classification_outcomes ( confusion_matrix : pd . DataFrame , classes : Set [ Any ], class_name : str , ) -> Tuple [ int , int , int , int ]: \"\"\" Given a confusion matrix, this function counts the cases of: - **True Positives** : classifications that accurately labeled a class - **True Negatives** : classifications that accurately labeled an example as not belonging to a class. - **False Positives** : classifications that attributed the wrong label to an example. - **False Negatives** : classifications that falsely claimed that an example does not belong to a class. Args: confusion_matrix: The result of calling [generate_confusion_matrix] [toolbox.algorithms.learning.evaluation.generate_confusion_matrix] classes: The set of all class labels class_name: The name (label) of the class being evaluated. Returns: - `tp`: Count of True Positives - `tn`: Count of True Negatives - `fp`: Count of False Positives - `fn`: Count of False Negatives \"\"\" excl_idx = classes . difference ( set (( class_name ,))) tp = confusion_matrix . loc [ class_name , class_name ] tn = confusion_matrix . loc [ excl_idx , excl_idx ] . sum () . sum () fp = confusion_matrix . loc [ class_name , excl_idx ] . sum () fn = confusion_matrix . loc [ excl_idx , class_name ] . sum () return ( tp , tn , fp , fn )","title":"get_classification_outcomes()"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.evaluate_precision","text":"Precision, aka Positive Predictive Value (PPV). \\(PPV=\\dfrac{TP}{TP + FP}\\) Parameters: Name Type Description Default tp int True Positives required fp int False Positives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_precision ( tp : int , fp : int ) -> float : \"\"\"Precision, aka Positive Predictive Value (PPV). $PPV=\\dfrac{TP}{TP + FP}$ Args: tp: True Positives fp: False Positives \"\"\" try : return tp / ( tp + fp ) except ZeroDivisionError : return 0.0","title":"evaluate_precision()"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.evaluate_precision_neg","text":"Negative precision, aka Negative Predictive Value (NPV). \\(NPV=\\dfrac{TN}{TN + FN}\\) Parameters: Name Type Description Default tn int True Negatives required fn int False Negatives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_precision_neg ( tn : int , fn : int ) -> float : \"\"\"Negative precision, aka Negative Predictive Value (NPV). $NPV=\\dfrac{TN}{TN + FN}$ Args: tn: True Negatives fn: False Negatives \"\"\" try : return tn / ( tn + fn ) except ZeroDivisionError : return 0.0","title":"evaluate_precision_neg()"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.evaluate_sensitivity","text":"Sensitivity, aka Recall, aka True Positive Rate (TPR). \\(TPR=\\dfrac{TP}{TP + FN}\\) Parameters: Name Type Description Default tp int True Positives required fn int False Negatives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_sensitivity ( tp : int , fn : int ) -> float : \"\"\"Sensitivity, aka Recall, aka True Positive Rate (TPR). $TPR=\\dfrac{TP}{TP + FN}$ Args: tp: True Positives fn: False Negatives \"\"\" try : return tp / ( tp + fn ) except ZeroDivisionError : return 0.0","title":"evaluate_sensitivity()"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.evaluate_specificity","text":"Specificity, aka True Negative Rate (TNR). \\(TNR=\\dfrac{TP}{TP + FP}\\) Parameters: Name Type Description Default tp True Positives required fp int False Positives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_specificity ( tn : int , fp : int ) -> float : \"\"\"Specificity, aka True Negative Rate (TNR). $TNR=\\dfrac{TP}{TP + FP}$ Args: tp: True Positives fp: False Positives \"\"\" try : return tn / ( tn + fp ) except ZeroDivisionError : return 0.0","title":"evaluate_specificity()"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.evaluate_accuracy","text":"Accuracy (ACC). \\(ACC=\\dfrac{TP + TN}{TP + TN + FP + FN}\\) Parameters: Name Type Description Default tp int True Positives required tn int True Negatives required fp int False Positives required fn int False Negatives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_accuracy ( tp : int , tn : int , fp : int , fn : int ) -> float : \"\"\"Accuracy (ACC). $ACC=\\dfrac{TP + TN}{TP + TN + FP + FN}$ Args: tp: True Positives tn: True Negatives fp: False Positives fn: False Negatives \"\"\" try : return ( tp + tn ) / ( tp + tn + fp + fn ) except ZeroDivisionError : return 0.0","title":"evaluate_accuracy()"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.evaluate_f1","text":"F1-score. F1-score \\(=\\dfrac{2TP}{2TP + FP + FN}\\) Parameters: Name Type Description Default tp int True Positives required fp int False Positives required fn int False Negatives required Source code in toolbox/algorithms/learning/evaluation.py def evaluate_f1 ( tp : int , fp : int , fn : int ) -> float : \"\"\"F1-score. *F1-score* $=\\dfrac{2TP}{2TP + FP + FN}$ Args: tp: True Positives fp: False Positives fn: False Negatives \"\"\" try : return 2 * tp / ( 2 * tp + fp + fn ) except ZeroDivisionError : return 0.0","title":"evaluate_f1()"},{"location":"machine-learning/evaluation/#toolbox.algorithms.learning.evaluation.evaluate_all","text":"Evaluates the following statistical measures: precision negative precision sensitivity (recall) specificity accuracy f1-score Parameters: Name Type Description Default tp int True Positives required tn int True Negatives required fp int False Positives required fn int False Negatives required Returns: Type Description dict See source for dictionary keys. Source code in toolbox/algorithms/learning/evaluation.py def evaluate_all ( tp : int , tn : int , fp : int , fn : int ) -> dict [ str , float ]: \"\"\" Evaluates the following statistical measures: - [precision] [toolbox.algorithms.learning.evaluation.evaluate_precision] - [negative precision] [toolbox.algorithms.learning.evaluation.evaluate_precision_neg] - [sensitivity (recall)] [toolbox.algorithms.learning.evaluation.evaluate_sensitivity] - [specificity] [toolbox.algorithms.learning.evaluation.evaluate_specificity] - [accuracy] [toolbox.algorithms.learning.evaluation.evaluate_accuracy] - [f1-score] [toolbox.algorithms.learning.evaluation.evaluate_f1] Args: tp: True Positives tn: True Negatives fp: False Positives fn: False Negatives Returns: See source for dictionary keys. \"\"\" return { \"True Positives\" : tp , \"True Negatives\" : tn , \"False Positives\" : fp , \"False Negatives\" : fn , \"Precision\" : evaluate_precision ( tp , fp ), \"Precision negative\" : evaluate_precision_neg ( tn , fn ), \"Sensitivity\" : evaluate_sensitivity ( tp , fn ), \"Specificity\" : evaluate_specificity ( tn , fp ), \"Accuracy\" : evaluate_accuracy ( tp , tn , fp , fn ), \"F1-score\" : evaluate_f1 ( tp , fp , fn ), }","title":"evaluate_all()"},{"location":"optimization/particle-swarm/","text":"Particle Swarm Optimization pso ( fitness_func , n_particles , search_space , inertia , cognitive , social , max_steps , min_delta = 0 , min_delta_repeats = 10 , verbose = True ) This implementation of particle-swarm optimization starts by initializing a number of particles uniformally randomly across the search space. The search space defines the range of possible values to search for in each of the dimensions of the space. See generate_swarm . Subsequently, at each iteration : every particle measures its fitness against the given fitness function. See Particle.evaluate_fitness . the best position - the one for which the fitness function yields the minimal value - is found and broadcast to the entire swarm. Note that this is an implementation of the Global topology , in which each particle is able to communicate with all other particles of the swarm. There are other topologies which may be less prone to particles getting stuck in local minima. See evaluate_swarm . given the swarm's best position, each particle moves towards it by a degree dependent on the optimization parameters (described below), adjusted by a random factor. See update_swarm . The position update (step 3 above) is dependent on the three parameters : inertia cognitive social The three steps above are repeated until the stopping condition is met. Here, the stopping condition is met if either : max_steps is reached, or there is a min_delta or smaller change in the swarm's best position repeatedly for min_delta_repeats steps. Parameters: Name Type Description Default fitness_func Callable The function to minimize required n_particles int More particles will slow the performance but will increase the likelihood of finding a global minimum required search_space List[Tuple[float, float]] A range, for each dimension, within which to search required inertia float A constant in the range \\([0, 1]\\) which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage exploitation . Higher values encourage exploration . required cognitive float A constant in the range \\(]0, 2[\\) , represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour exploitation . required social float A constant in the range \\(]0, 2[\\) , represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour exploration . required max_steps int A stopping condition. required min_delta float In combination with min_delta_repeats defines a stopping condition based on the change in distance travelled across the search space between steps. 0 min_delta_repeats int The number of times a value smaller than min_delta is observed before the optimization is stopped. 10 verbose bool Set to False to silence printing at each step. True Returns: Type Description List[float] The final best position, and hopefully the global minimum. Examples: >>> def booth ( x , y ): return ( x + 2 * y - 7 ) ** 2 + ( 2 * x + y - 5 ) ** 2 >>> x , y = pso ( fitness_func = booth , n_particles = 1000 , search_space = [( - 10 , 10 ), ( - 10 , 10 )], inertia = 0.2 , cognitive = 0.4 , social = 0.6 , max_steps = 100 , min_delta = 0.00001 , min_delta_repeats = 10 , verbose = True ) 1 [0.90367590, 3.07077955] 0.20555572 2 [0.90864515, 3.07453398] 0.00000000 3 [0.91783424, 3.14256790] 0.09377374 4 [1.01895113, 2.98260133] 0.17020089 5 [1.00240149, 3.05197469] 0.05203608 6 [0.98202492, 2.98871176] 0.06888439 7 [0.99610861, 2.99957312] 0.01907586 8 [0.99225189, 3.00850750] 0.01124424 9 [0.99971696, 3.00322148] 0.00939563 10 [1.00077373, 3.00156704] 0.00204074 11 [1.00038588, 2.99972079] 0.00187193 12 [1.00008264, 2.99990549] 0.00026376 13 [0.99999774, 2.99996748] 0.00009338 14 [1.00004893, 2.99993361] 0.00004135 15 [1.00008673, 2.99996720] 0.00006891 16 [1.00000604, 2.99997651] 0.00006335 17 [1.00002732, 2.99998583] 0.00002063 18 [1.00000184, 3.00000745] 0.00002983 19 [0.99999714, 2.99999854] 0.00000854 20 [0.99999807, 2.99999881] 0.00000145 21 [1.00000006, 2.99999973] 0.00000165 22 [1.00000015, 3.00000011] 0.00000038 23 [0.99999995, 2.99999995] 0.00000016 24 [1.00000003, 3.00000010] 0.00000016 25 [1.00000002, 3.00000002] 0.00000008 26 [1.00000003, 2.99999998] 0.00000003 27 [1.00000000, 3.00000000] 0.00000002 28 [1.00000000, 3.00000000] 0.00000000 Source code in toolbox/algorithms/optimization/swarm.py def pso ( fitness_func : Callable , n_particles : int , search_space : List [ Tuple [ float , float ]], inertia : float , cognitive : float , social : float , max_steps : int , min_delta : float = 0 , min_delta_repeats : int = 10 , verbose : bool = True , ) -> List [ float ]: \"\"\" This implementation of particle-swarm optimization starts by initializing a number of particles uniformally randomly across the search space. The search space defines the range of possible values to search for in each of the dimensions of the space. See [generate_swarm] [toolbox.algorithms.optimization.swarm.generate_swarm]. Subsequently, at each iteration : 1. every particle measures its fitness against the given fitness function. See [Particle.evaluate_fitness] [toolbox.algorithms.optimization.swarm.Particle.evaluate_fitness]. 2. the best position - the one for which the fitness function yields the minimal value - is found and broadcast to the entire swarm. Note that this is an implementation of the **Global topology**, in which each particle is able to communicate with all other particles of the swarm. There are other topologies which may be less prone to particles getting stuck in local minima. See [evaluate_swarm] [toolbox.algorithms.optimization.swarm.evaluate_swarm]. 3. given the swarm's best position, each particle moves towards it by a degree dependent on the optimization parameters (described below), adjusted by a random factor. See [update_swarm] [toolbox.algorithms.optimization.swarm.update_swarm]. The position update (step 3 above) is dependent on the three parameters : - `inertia` - `cognitive` - `social` The three steps above are repeated until the stopping condition is met. Here, the stopping condition is met if either : - `max_steps` is reached, or - there is a `min_delta` or smaller change in the swarm's best position repeatedly for `min_delta_repeats` steps. Args: fitness_func: The function to minimize n_particles: More particles will slow the performance but will increase the likelihood of finding a global minimum search_space: A range, for each dimension, within which to search inertia: A constant in the range $[0, 1]$ which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage *exploitation*. Higher values encourage *exploration*. cognitive: A constant in the range $]0, 2[$, represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour *exploitation*. social: A constant in the range $]0, 2[$, represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour *exploration*. max_steps: A stopping condition. min_delta: In combination with `min_delta_repeats` defines a stopping condition based on the change in distance travelled across the search space between steps. min_delta_repeats: The number of times a value smaller than `min_delta` is observed before the optimization is stopped. verbose: Set to False to silence printing at each step. Returns: The final best position, and hopefully the global minimum. Examples: >>> def booth(x, y): return (x + 2*y - 7)**2 + (2*x + y - 5)**2 >>> x, y = pso( fitness_func=booth, n_particles=1000, search_space=[(-10, 10), (-10, 10)], inertia=0.2, cognitive=0.4, social=0.6, max_steps=100, min_delta=0.00001, min_delta_repeats=10, verbose=True ) 1 [0.90367590, 3.07077955] 0.20555572 2 [0.90864515, 3.07453398] 0.00000000 3 [0.91783424, 3.14256790] 0.09377374 4 [1.01895113, 2.98260133] 0.17020089 5 [1.00240149, 3.05197469] 0.05203608 6 [0.98202492, 2.98871176] 0.06888439 7 [0.99610861, 2.99957312] 0.01907586 8 [0.99225189, 3.00850750] 0.01124424 9 [0.99971696, 3.00322148] 0.00939563 10 [1.00077373, 3.00156704] 0.00204074 11 [1.00038588, 2.99972079] 0.00187193 12 [1.00008264, 2.99990549] 0.00026376 13 [0.99999774, 2.99996748] 0.00009338 14 [1.00004893, 2.99993361] 0.00004135 15 [1.00008673, 2.99996720] 0.00006891 16 [1.00000604, 2.99997651] 0.00006335 17 [1.00002732, 2.99998583] 0.00002063 18 [1.00000184, 3.00000745] 0.00002983 19 [0.99999714, 2.99999854] 0.00000854 20 [0.99999807, 2.99999881] 0.00000145 21 [1.00000006, 2.99999973] 0.00000165 22 [1.00000015, 3.00000011] 0.00000038 23 [0.99999995, 2.99999995] 0.00000016 24 [1.00000003, 3.00000010] 0.00000016 25 [1.00000002, 3.00000002] 0.00000008 26 [1.00000003, 2.99999998] 0.00000003 27 [1.00000000, 3.00000000] 0.00000002 28 [1.00000000, 3.00000000] 0.00000000 \"\"\" if n_particles < 2 : raise ValueError ( \"the swarm must count at least two particles\" ) if max_steps < 1 : raise ValueError ( \"there should be at least one step\" ) if min_delta < 0 : raise ValueError ( \"absolute delta should be non-negative (0 to reach max_steps)\" ) swarm = generate_swarm ( n_particles = n_particles , search_space = search_space , fitness_func = fitness_func , inertia = inertia , cognitive = cognitive , social = social , ) delta = 0 delta_count = 0 step = 1 best_particle = update_swarm ( swarm ) best_position = best_particle . position while step < max_steps : best_particle = update_swarm ( swarm ) delta = distance ( best_position , best_particle . position ) best_position = best_particle . position if verbose : position_str = \", \" . join ([ f \" { d : .8f } \" for d in best_position ]) print ( f \" { step } \\t [ { position_str } ] \\t { delta : .8f } \" ) if delta <= min_delta : delta_count += 1 if delta_count >= min_delta_repeats : break else : delta_count = 0 step += 1 return best_position generate_swarm ( n_particles , search_space , fitness_func , inertia , cognitive , social ) This is the initialization step of the PSO algorithm, it is called only once. Parameters: Name Type Description Default n_particles int More particles will slow the performance but will increase the likelihood of finding a global minimum required search_space List[Tuple[float, float]] A range, for each dimension, within which to search required fitness_func Callable The function to minimize required inertia float A constant in the range [0, 1] which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage exploitation . Higher values encourage exploration . required cognitive float A constant in the range ]0, 2[, represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour exploitation . required social float A constant in the range ]0, 2[, represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour exploration . required Returns: Type Description List[toolbox.algorithms.optimization.swarm.Particle] A list of Particles Source code in toolbox/algorithms/optimization/swarm.py def generate_swarm ( n_particles : int , search_space : List [ Tuple [ float , float ]], fitness_func : Callable , inertia : float , cognitive : float , social : float , ) -> List [ Particle ]: \"\"\" This is the initialization step of the PSO algorithm, it is called only once. Args: n_particles: More particles will slow the performance but will increase the likelihood of finding a global minimum search_space: A range, for each dimension, within which to search fitness_func: The function to minimize inertia: A constant in the range [0, 1] which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage *exploitation*. Higher values encourage *exploration*. cognitive: A constant in the range ]0, 2[, represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour *exploitation*. social: A constant in the range ]0, 2[, represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour *exploration*. Returns: A list of [Particles][toolbox.algorithms.optimization.swarm.Particle] \"\"\" positions = array ( [ uniform ( lo , hi , n_particles ) for lo , hi in search_space ] ) . transpose () particles = [] for position in positions : particles . append ( Particle ( position = position , search_space = search_space , fitness_func = fitness_func , inertia = inertia , cognitive = cognitive , social = social , ) ) return particles evaluate_swarm ( swarm ) At each iteration (step) of the PSO algorithm, the particles evaluate their fitness values by calling their Particle.evaluate_fitness methods. The particle with the best fitness determines the swarm's best position for that iteration. Returns: Type Description Particle The particle with the best position. Source code in toolbox/algorithms/optimization/swarm.py def evaluate_swarm ( swarm : List [ Particle ]) -> Particle : \"\"\" At each iteration (step) of the PSO algorithm, the particles evaluate their fitness values by calling their [Particle.evaluate_fitness] [toolbox.algorithms.optimization.swarm.Particle.evaluate_fitness] methods. The particle with the best fitness determines the swarm's best position for that iteration. Args: A list of [Particles][toolbox.algorithms.optimization.swarm.Particle] belonging to the swarm. Returns: The particle with the best position. \"\"\" best_particle = swarm [ 0 ] best_fitness = best_particle . fitness for particle in swarm : particle . evaluate_fitness () if particle . fitness < best_fitness : best_fitness = particle . fitness best_particle = particle return best_particle update_swarm ( swarm ) At each iteration (step) of the PSO algorithm, after discovering the swarm's best position with evaluate_swarm , the particles update their positions by calling their Particle.update methods. Returns: Type Description Particle The particle with the best position. Source code in toolbox/algorithms/optimization/swarm.py def update_swarm ( swarm : List [ Particle ]) -> Particle : \"\"\" At each iteration (step) of the PSO algorithm, after discovering the swarm's best position with [evaluate_swarm] [toolbox.algorithms.optimization.swarm.evaluate_swarm], the particles update their positions by calling their [Particle.update] [toolbox.algorithms.optimization.swarm.Particle.evaluate_fitness] methods. Args: A list of [Particles][toolbox.algorithms.optimization.swarm.Particle] belonging to the swarm. Returns: The particle with the best position. \"\"\" best_particle = evaluate_swarm ( swarm ) for particle in swarm : particle . update ( best_particle . position ) return best_particle Particle dataclass Parameters: Name Type Description Default position List[float] The particle's current position within the search space. required search_space List[Tuple[float, float]] see pso. required inertia float see pso. required cognitive float see pso. required evaluate_fitness ( self ) Particles evaluate their fitness simply by applying the fitness function to their current position. They keep track of their historical best position, which has an effect on their update velocity. Source code in toolbox/algorithms/optimization/swarm.py def evaluate_fitness ( self ) -> None : \"\"\" Particles evaluate their fitness simply by applying the fitness function to their current position. They keep track of their historical best position, which has an effect on their update velocity. \"\"\" self . fitness = self . fitness_func ( * self . position ) if self . fitness < self . fitness_best : self . fitness_best = self . fitness self . position_best = self . position update ( self , swarm_best_pos ) This is the core of the PSO algorithm. At each iteration (step), every particle updates its position in the search space according to the swarm's current best position, that is the position of the particle with the best fitness. The extent to which each particle moves towards the best position, its update velocity, is determined by three key constants, the definitions of which are worth repeating here : inertia : A constant in the range \\([0, 1]\\) which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage exploitation . Higher values encourage exploration . cognitive : A constant in the range \\(]0, 2[\\) , represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour exploitation . social : A constant in the range \\(]0, 2[\\) , represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour exploration . Parameters: Name Type Description Default swarm_best_pos List[float] The current position of the Particle with the best fitness. required Source code in toolbox/algorithms/optimization/swarm.py def update ( self , swarm_best_pos : List [ float ]) -> None : \"\"\" This is the core of the PSO algorithm. At each iteration (step), every particle updates its position in the search space according to the swarm's current best position, that is the position of the particle with the best fitness. The extent to which each particle moves towards the best position, its update velocity, is determined by three key constants, the definitions of which are worth repeating here : - **inertia** : A constant in the range $[0, 1]$ which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage *exploitation*. Higher values encourage *exploration*. - **cognitive** : A constant in the range $]0, 2[$, represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour *exploitation*. - **social** : A constant in the range $]0, 2[$, represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour *exploration*. Args: swarm_best_pos: The current position of the Particle with the best fitness. \"\"\" for ix , _d in enumerate ( self . position ): inertia = self . inertia * self . velocity [ ix ] cognitive_acc = self . cognitive * uniform ( 0 , 1 ) cognitive_pos = cognitive_acc * ( self . position_best [ ix ] - self . position [ ix ]) social_acc = self . social * uniform ( 0 , 1 ) social_pos = social_acc * ( swarm_best_pos [ ix ] - self . position [ ix ]) self . velocity [ ix ] = inertia + cognitive_pos + social_pos self . position [ ix ] = self . position [ ix ] + self . velocity [ ix ]","title":"Particle-swarm"},{"location":"optimization/particle-swarm/#particle-swarm-optimization","text":"","title":"Particle Swarm Optimization"},{"location":"optimization/particle-swarm/#toolbox.algorithms.optimization.swarm.pso","text":"This implementation of particle-swarm optimization starts by initializing a number of particles uniformally randomly across the search space. The search space defines the range of possible values to search for in each of the dimensions of the space. See generate_swarm . Subsequently, at each iteration : every particle measures its fitness against the given fitness function. See Particle.evaluate_fitness . the best position - the one for which the fitness function yields the minimal value - is found and broadcast to the entire swarm. Note that this is an implementation of the Global topology , in which each particle is able to communicate with all other particles of the swarm. There are other topologies which may be less prone to particles getting stuck in local minima. See evaluate_swarm . given the swarm's best position, each particle moves towards it by a degree dependent on the optimization parameters (described below), adjusted by a random factor. See update_swarm . The position update (step 3 above) is dependent on the three parameters : inertia cognitive social The three steps above are repeated until the stopping condition is met. Here, the stopping condition is met if either : max_steps is reached, or there is a min_delta or smaller change in the swarm's best position repeatedly for min_delta_repeats steps. Parameters: Name Type Description Default fitness_func Callable The function to minimize required n_particles int More particles will slow the performance but will increase the likelihood of finding a global minimum required search_space List[Tuple[float, float]] A range, for each dimension, within which to search required inertia float A constant in the range \\([0, 1]\\) which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage exploitation . Higher values encourage exploration . required cognitive float A constant in the range \\(]0, 2[\\) , represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour exploitation . required social float A constant in the range \\(]0, 2[\\) , represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour exploration . required max_steps int A stopping condition. required min_delta float In combination with min_delta_repeats defines a stopping condition based on the change in distance travelled across the search space between steps. 0 min_delta_repeats int The number of times a value smaller than min_delta is observed before the optimization is stopped. 10 verbose bool Set to False to silence printing at each step. True Returns: Type Description List[float] The final best position, and hopefully the global minimum. Examples: >>> def booth ( x , y ): return ( x + 2 * y - 7 ) ** 2 + ( 2 * x + y - 5 ) ** 2 >>> x , y = pso ( fitness_func = booth , n_particles = 1000 , search_space = [( - 10 , 10 ), ( - 10 , 10 )], inertia = 0.2 , cognitive = 0.4 , social = 0.6 , max_steps = 100 , min_delta = 0.00001 , min_delta_repeats = 10 , verbose = True ) 1 [0.90367590, 3.07077955] 0.20555572 2 [0.90864515, 3.07453398] 0.00000000 3 [0.91783424, 3.14256790] 0.09377374 4 [1.01895113, 2.98260133] 0.17020089 5 [1.00240149, 3.05197469] 0.05203608 6 [0.98202492, 2.98871176] 0.06888439 7 [0.99610861, 2.99957312] 0.01907586 8 [0.99225189, 3.00850750] 0.01124424 9 [0.99971696, 3.00322148] 0.00939563 10 [1.00077373, 3.00156704] 0.00204074 11 [1.00038588, 2.99972079] 0.00187193 12 [1.00008264, 2.99990549] 0.00026376 13 [0.99999774, 2.99996748] 0.00009338 14 [1.00004893, 2.99993361] 0.00004135 15 [1.00008673, 2.99996720] 0.00006891 16 [1.00000604, 2.99997651] 0.00006335 17 [1.00002732, 2.99998583] 0.00002063 18 [1.00000184, 3.00000745] 0.00002983 19 [0.99999714, 2.99999854] 0.00000854 20 [0.99999807, 2.99999881] 0.00000145 21 [1.00000006, 2.99999973] 0.00000165 22 [1.00000015, 3.00000011] 0.00000038 23 [0.99999995, 2.99999995] 0.00000016 24 [1.00000003, 3.00000010] 0.00000016 25 [1.00000002, 3.00000002] 0.00000008 26 [1.00000003, 2.99999998] 0.00000003 27 [1.00000000, 3.00000000] 0.00000002 28 [1.00000000, 3.00000000] 0.00000000 Source code in toolbox/algorithms/optimization/swarm.py def pso ( fitness_func : Callable , n_particles : int , search_space : List [ Tuple [ float , float ]], inertia : float , cognitive : float , social : float , max_steps : int , min_delta : float = 0 , min_delta_repeats : int = 10 , verbose : bool = True , ) -> List [ float ]: \"\"\" This implementation of particle-swarm optimization starts by initializing a number of particles uniformally randomly across the search space. The search space defines the range of possible values to search for in each of the dimensions of the space. See [generate_swarm] [toolbox.algorithms.optimization.swarm.generate_swarm]. Subsequently, at each iteration : 1. every particle measures its fitness against the given fitness function. See [Particle.evaluate_fitness] [toolbox.algorithms.optimization.swarm.Particle.evaluate_fitness]. 2. the best position - the one for which the fitness function yields the minimal value - is found and broadcast to the entire swarm. Note that this is an implementation of the **Global topology**, in which each particle is able to communicate with all other particles of the swarm. There are other topologies which may be less prone to particles getting stuck in local minima. See [evaluate_swarm] [toolbox.algorithms.optimization.swarm.evaluate_swarm]. 3. given the swarm's best position, each particle moves towards it by a degree dependent on the optimization parameters (described below), adjusted by a random factor. See [update_swarm] [toolbox.algorithms.optimization.swarm.update_swarm]. The position update (step 3 above) is dependent on the three parameters : - `inertia` - `cognitive` - `social` The three steps above are repeated until the stopping condition is met. Here, the stopping condition is met if either : - `max_steps` is reached, or - there is a `min_delta` or smaller change in the swarm's best position repeatedly for `min_delta_repeats` steps. Args: fitness_func: The function to minimize n_particles: More particles will slow the performance but will increase the likelihood of finding a global minimum search_space: A range, for each dimension, within which to search inertia: A constant in the range $[0, 1]$ which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage *exploitation*. Higher values encourage *exploration*. cognitive: A constant in the range $]0, 2[$, represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour *exploitation*. social: A constant in the range $]0, 2[$, represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour *exploration*. max_steps: A stopping condition. min_delta: In combination with `min_delta_repeats` defines a stopping condition based on the change in distance travelled across the search space between steps. min_delta_repeats: The number of times a value smaller than `min_delta` is observed before the optimization is stopped. verbose: Set to False to silence printing at each step. Returns: The final best position, and hopefully the global minimum. Examples: >>> def booth(x, y): return (x + 2*y - 7)**2 + (2*x + y - 5)**2 >>> x, y = pso( fitness_func=booth, n_particles=1000, search_space=[(-10, 10), (-10, 10)], inertia=0.2, cognitive=0.4, social=0.6, max_steps=100, min_delta=0.00001, min_delta_repeats=10, verbose=True ) 1 [0.90367590, 3.07077955] 0.20555572 2 [0.90864515, 3.07453398] 0.00000000 3 [0.91783424, 3.14256790] 0.09377374 4 [1.01895113, 2.98260133] 0.17020089 5 [1.00240149, 3.05197469] 0.05203608 6 [0.98202492, 2.98871176] 0.06888439 7 [0.99610861, 2.99957312] 0.01907586 8 [0.99225189, 3.00850750] 0.01124424 9 [0.99971696, 3.00322148] 0.00939563 10 [1.00077373, 3.00156704] 0.00204074 11 [1.00038588, 2.99972079] 0.00187193 12 [1.00008264, 2.99990549] 0.00026376 13 [0.99999774, 2.99996748] 0.00009338 14 [1.00004893, 2.99993361] 0.00004135 15 [1.00008673, 2.99996720] 0.00006891 16 [1.00000604, 2.99997651] 0.00006335 17 [1.00002732, 2.99998583] 0.00002063 18 [1.00000184, 3.00000745] 0.00002983 19 [0.99999714, 2.99999854] 0.00000854 20 [0.99999807, 2.99999881] 0.00000145 21 [1.00000006, 2.99999973] 0.00000165 22 [1.00000015, 3.00000011] 0.00000038 23 [0.99999995, 2.99999995] 0.00000016 24 [1.00000003, 3.00000010] 0.00000016 25 [1.00000002, 3.00000002] 0.00000008 26 [1.00000003, 2.99999998] 0.00000003 27 [1.00000000, 3.00000000] 0.00000002 28 [1.00000000, 3.00000000] 0.00000000 \"\"\" if n_particles < 2 : raise ValueError ( \"the swarm must count at least two particles\" ) if max_steps < 1 : raise ValueError ( \"there should be at least one step\" ) if min_delta < 0 : raise ValueError ( \"absolute delta should be non-negative (0 to reach max_steps)\" ) swarm = generate_swarm ( n_particles = n_particles , search_space = search_space , fitness_func = fitness_func , inertia = inertia , cognitive = cognitive , social = social , ) delta = 0 delta_count = 0 step = 1 best_particle = update_swarm ( swarm ) best_position = best_particle . position while step < max_steps : best_particle = update_swarm ( swarm ) delta = distance ( best_position , best_particle . position ) best_position = best_particle . position if verbose : position_str = \", \" . join ([ f \" { d : .8f } \" for d in best_position ]) print ( f \" { step } \\t [ { position_str } ] \\t { delta : .8f } \" ) if delta <= min_delta : delta_count += 1 if delta_count >= min_delta_repeats : break else : delta_count = 0 step += 1 return best_position","title":"pso()"},{"location":"optimization/particle-swarm/#toolbox.algorithms.optimization.swarm.generate_swarm","text":"This is the initialization step of the PSO algorithm, it is called only once. Parameters: Name Type Description Default n_particles int More particles will slow the performance but will increase the likelihood of finding a global minimum required search_space List[Tuple[float, float]] A range, for each dimension, within which to search required fitness_func Callable The function to minimize required inertia float A constant in the range [0, 1] which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage exploitation . Higher values encourage exploration . required cognitive float A constant in the range ]0, 2[, represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour exploitation . required social float A constant in the range ]0, 2[, represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour exploration . required Returns: Type Description List[toolbox.algorithms.optimization.swarm.Particle] A list of Particles Source code in toolbox/algorithms/optimization/swarm.py def generate_swarm ( n_particles : int , search_space : List [ Tuple [ float , float ]], fitness_func : Callable , inertia : float , cognitive : float , social : float , ) -> List [ Particle ]: \"\"\" This is the initialization step of the PSO algorithm, it is called only once. Args: n_particles: More particles will slow the performance but will increase the likelihood of finding a global minimum search_space: A range, for each dimension, within which to search fitness_func: The function to minimize inertia: A constant in the range [0, 1] which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage *exploitation*. Higher values encourage *exploration*. cognitive: A constant in the range ]0, 2[, represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour *exploitation*. social: A constant in the range ]0, 2[, represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour *exploration*. Returns: A list of [Particles][toolbox.algorithms.optimization.swarm.Particle] \"\"\" positions = array ( [ uniform ( lo , hi , n_particles ) for lo , hi in search_space ] ) . transpose () particles = [] for position in positions : particles . append ( Particle ( position = position , search_space = search_space , fitness_func = fitness_func , inertia = inertia , cognitive = cognitive , social = social , ) ) return particles","title":"generate_swarm()"},{"location":"optimization/particle-swarm/#toolbox.algorithms.optimization.swarm.evaluate_swarm","text":"At each iteration (step) of the PSO algorithm, the particles evaluate their fitness values by calling their Particle.evaluate_fitness methods. The particle with the best fitness determines the swarm's best position for that iteration. Returns: Type Description Particle The particle with the best position. Source code in toolbox/algorithms/optimization/swarm.py def evaluate_swarm ( swarm : List [ Particle ]) -> Particle : \"\"\" At each iteration (step) of the PSO algorithm, the particles evaluate their fitness values by calling their [Particle.evaluate_fitness] [toolbox.algorithms.optimization.swarm.Particle.evaluate_fitness] methods. The particle with the best fitness determines the swarm's best position for that iteration. Args: A list of [Particles][toolbox.algorithms.optimization.swarm.Particle] belonging to the swarm. Returns: The particle with the best position. \"\"\" best_particle = swarm [ 0 ] best_fitness = best_particle . fitness for particle in swarm : particle . evaluate_fitness () if particle . fitness < best_fitness : best_fitness = particle . fitness best_particle = particle return best_particle","title":"evaluate_swarm()"},{"location":"optimization/particle-swarm/#toolbox.algorithms.optimization.swarm.update_swarm","text":"At each iteration (step) of the PSO algorithm, after discovering the swarm's best position with evaluate_swarm , the particles update their positions by calling their Particle.update methods. Returns: Type Description Particle The particle with the best position. Source code in toolbox/algorithms/optimization/swarm.py def update_swarm ( swarm : List [ Particle ]) -> Particle : \"\"\" At each iteration (step) of the PSO algorithm, after discovering the swarm's best position with [evaluate_swarm] [toolbox.algorithms.optimization.swarm.evaluate_swarm], the particles update their positions by calling their [Particle.update] [toolbox.algorithms.optimization.swarm.Particle.evaluate_fitness] methods. Args: A list of [Particles][toolbox.algorithms.optimization.swarm.Particle] belonging to the swarm. Returns: The particle with the best position. \"\"\" best_particle = evaluate_swarm ( swarm ) for particle in swarm : particle . update ( best_particle . position ) return best_particle","title":"update_swarm()"},{"location":"optimization/particle-swarm/#toolbox.algorithms.optimization.swarm.Particle","text":"Parameters: Name Type Description Default position List[float] The particle's current position within the search space. required search_space List[Tuple[float, float]] see pso. required inertia float see pso. required cognitive float see pso. required","title":"Particle"},{"location":"optimization/particle-swarm/#toolbox.algorithms.optimization.swarm.Particle.evaluate_fitness","text":"Particles evaluate their fitness simply by applying the fitness function to their current position. They keep track of their historical best position, which has an effect on their update velocity. Source code in toolbox/algorithms/optimization/swarm.py def evaluate_fitness ( self ) -> None : \"\"\" Particles evaluate their fitness simply by applying the fitness function to their current position. They keep track of their historical best position, which has an effect on their update velocity. \"\"\" self . fitness = self . fitness_func ( * self . position ) if self . fitness < self . fitness_best : self . fitness_best = self . fitness self . position_best = self . position","title":"evaluate_fitness()"},{"location":"optimization/particle-swarm/#toolbox.algorithms.optimization.swarm.Particle.update","text":"This is the core of the PSO algorithm. At each iteration (step), every particle updates its position in the search space according to the swarm's current best position, that is the position of the particle with the best fitness. The extent to which each particle moves towards the best position, its update velocity, is determined by three key constants, the definitions of which are worth repeating here : inertia : A constant in the range \\([0, 1]\\) which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage exploitation . Higher values encourage exploration . cognitive : A constant in the range \\(]0, 2[\\) , represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour exploitation . social : A constant in the range \\(]0, 2[\\) , represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour exploration . Parameters: Name Type Description Default swarm_best_pos List[float] The current position of the Particle with the best fitness. required Source code in toolbox/algorithms/optimization/swarm.py def update ( self , swarm_best_pos : List [ float ]) -> None : \"\"\" This is the core of the PSO algorithm. At each iteration (step), every particle updates its position in the search space according to the swarm's current best position, that is the position of the particle with the best fitness. The extent to which each particle moves towards the best position, its update velocity, is determined by three key constants, the definitions of which are worth repeating here : - **inertia** : A constant in the range $[0, 1]$ which determines the proportion by which each step's velocity will affect the change in position. Low values will slow the progress made at each step, and will potentially require more steps, but it will encourage *exploitation*. Higher values encourage *exploration*. - **cognitive** : A constant in the range $]0, 2[$, represents an individual particle's sense of certainty about its personal best position. A swarm with a high cognitive constant will favour *exploitation*. - **social** : A constant in the range $]0, 2[$, represents an individual particle's sense of certainty about the swarm's best position. A swarm with a high cognitive constant will favour *exploration*. Args: swarm_best_pos: The current position of the Particle with the best fitness. \"\"\" for ix , _d in enumerate ( self . position ): inertia = self . inertia * self . velocity [ ix ] cognitive_acc = self . cognitive * uniform ( 0 , 1 ) cognitive_pos = cognitive_acc * ( self . position_best [ ix ] - self . position [ ix ]) social_acc = self . social * uniform ( 0 , 1 ) social_pos = social_acc * ( swarm_best_pos [ ix ] - self . position [ ix ]) self . velocity [ ix ] = inertia + cognitive_pos + social_pos self . position [ ix ] = self . position [ ix ] + self . velocity [ ix ]","title":"update()"},{"location":"sorting/quicksort/","text":"Quicksort Comparison Can sort items of any type for which there is total ordering. Inplace Does not need memory resources for a second array of equal size. Stable Relative order of equal items is maintained. Lomuto quicksort_lomuto ( a , lo = 0 , hi = None ) Sorts the array \\(a\\) from \\(lo\\) to \\(hi\\) in place using Lomuto partitioning. Comparison Yes Inplace Yes Stable No \\(O(n^{2})\\) worst-case when A is already in sorted order. Parameters: Name Type Description Default a array-like The array required lo int The starting index of the section to be partitionned 0 hi Optional[int] The ending index of the section to be partitionned None This partitioning scheme (see partition_lomuto ) always selects the last element in the array as a pivot. It then runs through the array with two cursors \\(i\\) and \\(j\\) . \\(i\\) is initialized to \\(lo\\) . \\(j\\) runs through the array from \\(lo\\) to \\(hi\\) . When a value at index \\(j\\) is found to be smaller than the pivot, it is swapped with the value at index \\(i\\) and \\(i\\) is incremented by one. Finally, the pivot is swapped with the value at index \\(i\\) . Once these operations have run their course, it is certain that all values to the left of the pivot (which is now at index \\(i\\) ) are smaller than the pivot and that all values to the left are larger than the pivot. The index \\(i\\) serves as the partitioning (split) point for the next iteration of the sorting algorithm, at which point the array is split into the left and right parts of \\(a\\) and each part is partitionned again until the array is sorted. Examples: [5, 4, 3, 6, 2, 1, 9, 4, 0] --------------------------- 1 - [5, 4, 3, 6, 2, 1, 9, 4, 0] - [0, 4, 3, 6, 2, 1, 9, 4, 5] 2 - [4, 3, 2, 1, 4, 5, 6, 9] - [0, 4, 3, 2, 1, 4, 5, 6, 9] 3 - [3, 2, 1, 4, 4] - [0, 3, 2, 1, 4, 4, 5, 6, 9] 4 - [1, 2, 3] - [0, 1, 2, 3, 4, 4, 5, 6, 9] 5 - [2, 3] - [0, 1, 2, 3, 4, 4, 5, 6, 9] 6 - [6, 9] - [0, 1, 2, 3, 4, 4, 5, 6, 9] --------------------------- [0, 1, 2, 3, 4, 4, 5, 6, 9] Source code in toolbox/algorithms/sorting.py def quicksort_lomuto ( a , lo : int = 0 , hi : Optional [ int ] = None ) -> None : \"\"\"Sorts the array $a$ from $lo$ to $hi$ in place using Lomuto partitioning. | | | | --- | --- | | **Comparison** | Yes | | **Inplace** | Yes | | **Stable** | No | $O(n^{2})$ worst-case when A is already in sorted order. Args: a (array-like): The array lo (int): The starting index of the section to be partitionned hi (int): The ending index of the section to be partitionned This partitioning scheme (see [partition_lomuto] [toolbox.algorithms.sorting.partition_lomuto]) always selects the last element in the array as a pivot. It then runs through the array with two cursors $i$ and $j$. $i$ is initialized to $lo$. $j$ runs through the array from $lo$ to $hi$. When a value at index $j$ is found to be smaller than the pivot, it is swapped with the value at index $i$ and $i$ is incremented by one. Finally, the pivot is swapped with the value at index $i$. Once these operations have run their course, it is certain that all values to the left of the pivot (which is now at index $i$) are smaller than the pivot and that all values to the left are larger than the pivot. The index $i$ serves as the partitioning (split) point for the next iteration of the sorting algorithm, at which point the array is split into the left and right parts of $a$ and each part is partitionned again until the array is sorted. Examples: ``` [5, 4, 3, 6, 2, 1, 9, 4, 0] --------------------------- 1 - [5, 4, 3, 6, 2, 1, 9, 4, 0] - [0, 4, 3, 6, 2, 1, 9, 4, 5] 2 - [4, 3, 2, 1, 4, 5, 6, 9] - [0, 4, 3, 2, 1, 4, 5, 6, 9] 3 - [3, 2, 1, 4, 4] - [0, 3, 2, 1, 4, 4, 5, 6, 9] 4 - [1, 2, 3] - [0, 1, 2, 3, 4, 4, 5, 6, 9] 5 - [2, 3] - [0, 1, 2, 3, 4, 4, 5, 6, 9] 6 - [6, 9] - [0, 1, 2, 3, 4, 4, 5, 6, 9] --------------------------- [0, 1, 2, 3, 4, 4, 5, 6, 9] ``` \"\"\" n = len ( a ) if lo < 0 : raise ValueError ( \"lo must be non-negative\" ) if hi is None : hi = n - 1 elif hi >= n : raise ValueError ( \"hi must be within the index range\" ) if lo >= hi : return p = partition_lomuto ( a , lo , hi ) quicksort_lomuto ( a , lo , p - 1 ) quicksort_lomuto ( a , p + 1 , hi ) partition_lomuto ( a , lo , hi ) Lomuto partitioning. Parameters: Name Type Description Default a array-like The list which is being sorted required lo int The starting index of the section to be partitionned required hi int The ending index of the section to be partitionned required Returns: Type Description int int: The partition index Source code in toolbox/algorithms/sorting.py def partition_lomuto ( a , lo : int , hi : int ) -> int : \"\"\"Lomuto partitioning. Args: a (array-like): The list which is being sorted lo (int): The starting index of the section to be partitionned hi (int): The ending index of the section to be partitionned Returns: int: The partition index \"\"\" pivot = a [ hi ] i = lo for j in range ( lo , hi + 1 ): if a [ j ] < pivot : swap ( a , i , j ) i += 1 swap ( a , i , hi ) return i Hoare quicksort_hoare ( a , lo = 0 , hi = None ) Sorts the array a from lo to hi in place using Hoare partitioning. Comparison Yes Inplace Yes Stable No \\(O(n^{2})\\) worst-case when A is already in sorted order. Parameters: Name Type Description Default a array-like The array required lo int The starting index of the section to be partitionned 0 hi Optional[int] The ending index of the section to be partitionned None Examples: [5, 4, 3, 6, 2, 1, 9, 4, 0] --------------------------- 1 - [5, 4, 3, 6, 2, 1, 9, 4, 0] - [0, 1, 2, 6, 3, 4, 9, 4, 5] 2 - [0, 1, 2] - [0, 1, 2, 6, 3, 4, 9, 4, 5] 3 - [6, 3, 4, 9, 4, 5] - [0, 1, 2, 4, 3, 4, 9, 6, 5] 4 - [4, 3, 4] - [0, 1, 2, 3, 4, 4, 9, 6, 5] 5 - [9, 6, 5] - [0, 1, 2, 3, 4, 4, 5, 6, 9] --------------------------- [0, 1, 2, 3, 4, 4, 5, 6, 9] Source code in toolbox/algorithms/sorting.py def quicksort_hoare ( a , lo : int = 0 , hi : Optional [ int ] = None ) -> None : \"\"\"Sorts the array `a` from `lo` to `hi` in place using Hoare partitioning. | | | | --- | --- | | **Comparison** | Yes | | **Inplace** | Yes | | **Stable** | No | $O(n^{2})$ worst-case when A is already in sorted order. Args: a (array-like): The array lo (int): The starting index of the section to be partitionned hi (int): The ending index of the section to be partitionned Examples: ``` [5, 4, 3, 6, 2, 1, 9, 4, 0] --------------------------- 1 - [5, 4, 3, 6, 2, 1, 9, 4, 0] - [0, 1, 2, 6, 3, 4, 9, 4, 5] 2 - [0, 1, 2] - [0, 1, 2, 6, 3, 4, 9, 4, 5] 3 - [6, 3, 4, 9, 4, 5] - [0, 1, 2, 4, 3, 4, 9, 6, 5] 4 - [4, 3, 4] - [0, 1, 2, 3, 4, 4, 9, 6, 5] 5 - [9, 6, 5] - [0, 1, 2, 3, 4, 4, 5, 6, 9] --------------------------- [0, 1, 2, 3, 4, 4, 5, 6, 9] ``` \"\"\" n = len ( a ) if lo < 0 : raise ValueError ( \"lo must be non-negative\" ) if hi is None : hi = n - 1 elif hi >= n : raise ValueError ( \"hi must be within the index range\" ) if ( hi - lo ) < 2 : return p = partition_hoare ( a , lo , hi ) quicksort_hoare ( a , lo , p ) quicksort_hoare ( a , p + 1 , hi ) partition_hoare ( a , lo , hi ) Hoare partitioning. Parameters: Name Type Description Default a array-like The list which is being sorted required lo int The starting index of the section to be partitionned required hi int The ending index of the section to be partitionned required Returns: Type Description int int: The partition index Source code in toolbox/algorithms/sorting.py def partition_hoare ( a , lo : int , hi : int ) -> int : \"\"\"Hoare partitioning. Args: a (array-like): The list which is being sorted lo (int): The starting index of the section to be partitionned hi (int): The ending index of the section to be partitionned Returns: int: The partition index \"\"\" pivot = a [( lo + hi ) // 2 ] i = lo - 1 j = hi + 1 while True : while True : i += 1 if a [ i ] >= pivot : break while True : j -= 1 if a [ j ] <= pivot : break if i >= j : print ( a ) return j swap ( a , i , j )","title":"quicksort"},{"location":"sorting/quicksort/#quicksort","text":"Comparison Can sort items of any type for which there is total ordering. Inplace Does not need memory resources for a second array of equal size. Stable Relative order of equal items is maintained.","title":"Quicksort"},{"location":"sorting/quicksort/#lomuto","text":"","title":"Lomuto"},{"location":"sorting/quicksort/#toolbox.algorithms.sorting.quicksort_lomuto","text":"Sorts the array \\(a\\) from \\(lo\\) to \\(hi\\) in place using Lomuto partitioning. Comparison Yes Inplace Yes Stable No \\(O(n^{2})\\) worst-case when A is already in sorted order. Parameters: Name Type Description Default a array-like The array required lo int The starting index of the section to be partitionned 0 hi Optional[int] The ending index of the section to be partitionned None This partitioning scheme (see partition_lomuto ) always selects the last element in the array as a pivot. It then runs through the array with two cursors \\(i\\) and \\(j\\) . \\(i\\) is initialized to \\(lo\\) . \\(j\\) runs through the array from \\(lo\\) to \\(hi\\) . When a value at index \\(j\\) is found to be smaller than the pivot, it is swapped with the value at index \\(i\\) and \\(i\\) is incremented by one. Finally, the pivot is swapped with the value at index \\(i\\) . Once these operations have run their course, it is certain that all values to the left of the pivot (which is now at index \\(i\\) ) are smaller than the pivot and that all values to the left are larger than the pivot. The index \\(i\\) serves as the partitioning (split) point for the next iteration of the sorting algorithm, at which point the array is split into the left and right parts of \\(a\\) and each part is partitionned again until the array is sorted. Examples: [5, 4, 3, 6, 2, 1, 9, 4, 0] --------------------------- 1 - [5, 4, 3, 6, 2, 1, 9, 4, 0] - [0, 4, 3, 6, 2, 1, 9, 4, 5] 2 - [4, 3, 2, 1, 4, 5, 6, 9] - [0, 4, 3, 2, 1, 4, 5, 6, 9] 3 - [3, 2, 1, 4, 4] - [0, 3, 2, 1, 4, 4, 5, 6, 9] 4 - [1, 2, 3] - [0, 1, 2, 3, 4, 4, 5, 6, 9] 5 - [2, 3] - [0, 1, 2, 3, 4, 4, 5, 6, 9] 6 - [6, 9] - [0, 1, 2, 3, 4, 4, 5, 6, 9] --------------------------- [0, 1, 2, 3, 4, 4, 5, 6, 9] Source code in toolbox/algorithms/sorting.py def quicksort_lomuto ( a , lo : int = 0 , hi : Optional [ int ] = None ) -> None : \"\"\"Sorts the array $a$ from $lo$ to $hi$ in place using Lomuto partitioning. | | | | --- | --- | | **Comparison** | Yes | | **Inplace** | Yes | | **Stable** | No | $O(n^{2})$ worst-case when A is already in sorted order. Args: a (array-like): The array lo (int): The starting index of the section to be partitionned hi (int): The ending index of the section to be partitionned This partitioning scheme (see [partition_lomuto] [toolbox.algorithms.sorting.partition_lomuto]) always selects the last element in the array as a pivot. It then runs through the array with two cursors $i$ and $j$. $i$ is initialized to $lo$. $j$ runs through the array from $lo$ to $hi$. When a value at index $j$ is found to be smaller than the pivot, it is swapped with the value at index $i$ and $i$ is incremented by one. Finally, the pivot is swapped with the value at index $i$. Once these operations have run their course, it is certain that all values to the left of the pivot (which is now at index $i$) are smaller than the pivot and that all values to the left are larger than the pivot. The index $i$ serves as the partitioning (split) point for the next iteration of the sorting algorithm, at which point the array is split into the left and right parts of $a$ and each part is partitionned again until the array is sorted. Examples: ``` [5, 4, 3, 6, 2, 1, 9, 4, 0] --------------------------- 1 - [5, 4, 3, 6, 2, 1, 9, 4, 0] - [0, 4, 3, 6, 2, 1, 9, 4, 5] 2 - [4, 3, 2, 1, 4, 5, 6, 9] - [0, 4, 3, 2, 1, 4, 5, 6, 9] 3 - [3, 2, 1, 4, 4] - [0, 3, 2, 1, 4, 4, 5, 6, 9] 4 - [1, 2, 3] - [0, 1, 2, 3, 4, 4, 5, 6, 9] 5 - [2, 3] - [0, 1, 2, 3, 4, 4, 5, 6, 9] 6 - [6, 9] - [0, 1, 2, 3, 4, 4, 5, 6, 9] --------------------------- [0, 1, 2, 3, 4, 4, 5, 6, 9] ``` \"\"\" n = len ( a ) if lo < 0 : raise ValueError ( \"lo must be non-negative\" ) if hi is None : hi = n - 1 elif hi >= n : raise ValueError ( \"hi must be within the index range\" ) if lo >= hi : return p = partition_lomuto ( a , lo , hi ) quicksort_lomuto ( a , lo , p - 1 ) quicksort_lomuto ( a , p + 1 , hi )","title":"quicksort_lomuto()"},{"location":"sorting/quicksort/#toolbox.algorithms.sorting.partition_lomuto","text":"Lomuto partitioning. Parameters: Name Type Description Default a array-like The list which is being sorted required lo int The starting index of the section to be partitionned required hi int The ending index of the section to be partitionned required Returns: Type Description int int: The partition index Source code in toolbox/algorithms/sorting.py def partition_lomuto ( a , lo : int , hi : int ) -> int : \"\"\"Lomuto partitioning. Args: a (array-like): The list which is being sorted lo (int): The starting index of the section to be partitionned hi (int): The ending index of the section to be partitionned Returns: int: The partition index \"\"\" pivot = a [ hi ] i = lo for j in range ( lo , hi + 1 ): if a [ j ] < pivot : swap ( a , i , j ) i += 1 swap ( a , i , hi ) return i","title":"partition_lomuto()"},{"location":"sorting/quicksort/#hoare","text":"","title":"Hoare"},{"location":"sorting/quicksort/#toolbox.algorithms.sorting.quicksort_hoare","text":"Sorts the array a from lo to hi in place using Hoare partitioning. Comparison Yes Inplace Yes Stable No \\(O(n^{2})\\) worst-case when A is already in sorted order. Parameters: Name Type Description Default a array-like The array required lo int The starting index of the section to be partitionned 0 hi Optional[int] The ending index of the section to be partitionned None Examples: [5, 4, 3, 6, 2, 1, 9, 4, 0] --------------------------- 1 - [5, 4, 3, 6, 2, 1, 9, 4, 0] - [0, 1, 2, 6, 3, 4, 9, 4, 5] 2 - [0, 1, 2] - [0, 1, 2, 6, 3, 4, 9, 4, 5] 3 - [6, 3, 4, 9, 4, 5] - [0, 1, 2, 4, 3, 4, 9, 6, 5] 4 - [4, 3, 4] - [0, 1, 2, 3, 4, 4, 9, 6, 5] 5 - [9, 6, 5] - [0, 1, 2, 3, 4, 4, 5, 6, 9] --------------------------- [0, 1, 2, 3, 4, 4, 5, 6, 9] Source code in toolbox/algorithms/sorting.py def quicksort_hoare ( a , lo : int = 0 , hi : Optional [ int ] = None ) -> None : \"\"\"Sorts the array `a` from `lo` to `hi` in place using Hoare partitioning. | | | | --- | --- | | **Comparison** | Yes | | **Inplace** | Yes | | **Stable** | No | $O(n^{2})$ worst-case when A is already in sorted order. Args: a (array-like): The array lo (int): The starting index of the section to be partitionned hi (int): The ending index of the section to be partitionned Examples: ``` [5, 4, 3, 6, 2, 1, 9, 4, 0] --------------------------- 1 - [5, 4, 3, 6, 2, 1, 9, 4, 0] - [0, 1, 2, 6, 3, 4, 9, 4, 5] 2 - [0, 1, 2] - [0, 1, 2, 6, 3, 4, 9, 4, 5] 3 - [6, 3, 4, 9, 4, 5] - [0, 1, 2, 4, 3, 4, 9, 6, 5] 4 - [4, 3, 4] - [0, 1, 2, 3, 4, 4, 9, 6, 5] 5 - [9, 6, 5] - [0, 1, 2, 3, 4, 4, 5, 6, 9] --------------------------- [0, 1, 2, 3, 4, 4, 5, 6, 9] ``` \"\"\" n = len ( a ) if lo < 0 : raise ValueError ( \"lo must be non-negative\" ) if hi is None : hi = n - 1 elif hi >= n : raise ValueError ( \"hi must be within the index range\" ) if ( hi - lo ) < 2 : return p = partition_hoare ( a , lo , hi ) quicksort_hoare ( a , lo , p ) quicksort_hoare ( a , p + 1 , hi )","title":"quicksort_hoare()"},{"location":"sorting/quicksort/#toolbox.algorithms.sorting.partition_hoare","text":"Hoare partitioning. Parameters: Name Type Description Default a array-like The list which is being sorted required lo int The starting index of the section to be partitionned required hi int The ending index of the section to be partitionned required Returns: Type Description int int: The partition index Source code in toolbox/algorithms/sorting.py def partition_hoare ( a , lo : int , hi : int ) -> int : \"\"\"Hoare partitioning. Args: a (array-like): The list which is being sorted lo (int): The starting index of the section to be partitionned hi (int): The ending index of the section to be partitionned Returns: int: The partition index \"\"\" pivot = a [( lo + hi ) // 2 ] i = lo - 1 j = hi + 1 while True : while True : i += 1 if a [ i ] >= pivot : break while True : j -= 1 if a [ j ] <= pivot : break if i >= j : print ( a ) return j swap ( a , i , j )","title":"partition_hoare()"}]}